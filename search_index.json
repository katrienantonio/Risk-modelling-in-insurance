[
["index.html", "Risk Modelling in Insurance : a collection of computer labs in R 1 Intro", " Risk Modelling in Insurance : a collection of computer labs in R Katrien Antonio and Jonas Crevecoeur 2019-01-13 1 Intro This book is a collection of computer labs that come with the course on Risk Modelling in Insurance taught at University of Ljubljana (Slovenia) by Katrien Antonio (put together with the help of Jonas Crevecoeur). The labs are inspired by the book Loss Models: from data to decisions written by Stuart A. Klugman, Harry H. Panjer and Gordon E. Willmot and published by Wiley the book on Actuarial Modelling of Claim Counts by Michel Denuit et al., published by Wiley and the Modern Actuarial Risk Theory book by Rob Kaas et al., published by Springer We assume basic knowledge of working with objects and data sets in R, writing functions and running optimizations. "],
["simple-parametric-distributions-for-frequency-and-severity-data.html", "2 Simple, parametric distributions for frequency and severity data 2.1 The exponential distribution 2.2 Discrete distributions", " 2 Simple, parametric distributions for frequency and severity data 2.1 The exponential distribution In this tutorial you will simulate data from an exponential distribution with density \\[ f(x) = \\lambda \\cdot e^{-\\lambda \\cdot x}.\\] You will then explore and visualize these data. Finally, you will fit an exponential distribution to the data using Maximum Likelihood Estimation (MLE) (as discussed in Chapter 13 of the Loss Models book). 2.1.1 Simulating data Use the R function rexp to simulate 10 000 observations from an exponential distribution with mean \\(5\\). Create a variable nsim for the number of simulations; Create a variable lambda for the \\(\\lambda\\) value of the exponential distribution. Hint: the mean of the exponential distribution is given by \\(\\frac{1}{\\lambda}\\) when using the parametrization given above; Check the documentation of rexp to see which parametrization R uses for the exponential distribution; Simulate nsim observations from the exponential distribution. Store the result in the variable sim; Calculate mean(sim) and verify that the simulated mean is close to \\(5\\). # @1. nsim &lt;- 10000; # @2. lambda &lt;- 1/5; # @3. ?rexp # @4. sim &lt;- rexp(nsim, rate = lambda); # @5. mean(sim) [1] 4.98 2.1.2 Exploratory analysis Calculate the (empirical) variance of the simulated sample; Calculate the (emprical) skewness of the simulated sample. The skewness is defined as \\[ \\frac{E((X-\\mu)^3)}{\\sigma^3}; \\] Calculate (empirically) \\(VaR_{0.95}\\) and \\(TVaR_{0.95}\\) for the simulated sample. # @1. variance &lt;- var(sim) variance [1] 25.05 # @2. mu &lt;- mean(sim) sigma &lt;- sqrt(var(sim)) numerator &lt;- mean((sim - mu)^3) skewness &lt;- numerator / sigma^3 skewness [1] 2.032 # @3. var95 &lt;- quantile(sim, 0.95); var95 95% 14.83 tvar95 &lt;- mean(sim[sim &gt; var95]) tvar95 [1] 19.97 2.1.3 Data visualization with ggplot Load the package ggplot2; You will construct step-by-step the following graph of the empirical CDF Let \\(x_{(i)}\\) be the \\(i\\)-th simulated value when sorted ascending. The empirical CDF is given by \\[ \\hat{F}(x_{(i)}) = \\frac{\\# \\{\\text{observations } \\leq x_{(i)}\\}}{n} = \\frac{i}{n}.\\] 2.1. Create a new ggplot. Add stat_ecdf using the simulated data; ggplot() + stat_ecdf(aes(???)) 2.2. Change the color of the line to blue by adding the option col = #99CCFF to stat_ecdf; 2.3. Add the black and white ggplot theme, theme_bw(); 2.4. Add \\(x\\) and \\(y\\) labels to the graph. Hint: use xlab, ylab; 2.5. Add a vertical line to indicate the \\(VaR_{0.95}\\). Check the documentation for geom_vline; 2.6. Add a title to the plot using ggtitle; 2.7. Change the number of simulations nsim to 50 and observe the effect on the empirical CDF. Use geom_density to create a density plot of the data. Improve the look of the graph using what you learned when creating the plot of the empirical CDF. # @1 library(ggplot2) # @2.1 p &lt;- ggplot() + stat_ecdf(aes(sim)) # @2.2 p &lt;- ggplot() + stat_ecdf(aes(sim), col = &quot;#99CCFF&quot;) # @2.3 p &lt;- p + theme_bw() # @2.4 p &lt;- p + xlab(&#39;x&#39;) + ylab(&#39;F(x)&#39;) # @2.5 p &lt;- p + geom_vline(xintercept = var95, linetype = &#39;dashed&#39;) # @2.6 p &lt;- p + ggtitle(&#39;Empirical CDF&#39;) print(p) # @3 ggplot() + geom_density(aes(sim), fill = &quot;#99CCFF&quot;) + theme_bw() + ggtitle(&#39;Density&#39;) + xlab(&#39;x&#39;) 2.1.4 Maximum Likelihood Estimation (MLE) The density of the exponential distribution is given by \\[f(x) = \\lambda \\cdot e^{-\\lambda \\cdot x}.\\] You have nsim simulated observations \\(x_{i}, \\ldots, x_{nsim}\\) from this distribution. In this exercise you look for the MLE of the parameter \\(\\lambda\\) using the simulated data. Write down (on paper) the formula for the likelihood of the observed data as a function of \\(\\lambda\\); The likelihood for the observed data is \\[L(\\lambda) = \\prod_{i=1}^{\\text{nsim}} \\lambda \\cdot e^{-\\lambda \\cdot x_i}.\\] Derive (on paper) the loglikelihood; The loglikelihood for the observed data is \\[l(\\lambda) = \\sum_{i=1}^{\\text{nsim}} \\log(\\lambda) -\\lambda \\cdot x_i.\\] Compute (on paper) the MLE for \\(\\lambda\\). Hint: put \\(\\frac{d \\ell}{ d \\lambda}(\\hat{\\lambda}) = 0\\) and solve for the unknown \\(\\lambda\\); \\[\\frac{d l}{ d \\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^{\\text{nsim}} x_i\\] This derivative is zero when \\[ \\hat{\\lambda} = \\frac{\\text{nsim}}{\\sum_{i=1}^{\\text{nsim}} x_i}.\\] This is the MLE for \\(\\lambda\\). You will now find the MLE numerically in R by optimizing the likelihood using the nlm procedure; 4.1. Define an R-function loglikelihood which takes as input \\(\\lambda\\) and returns the loglikelihood for the simulated sample; loglikelihood &lt;- function(lambda) { loglikelihood &lt;- ??? return(loglikelihood) } 4.2. The nlm procedure minimizes a function. You will minimize the negative loglikelihood \\(-l(\\lambda)\\) to find the maximum likelihood estimator \\(\\hat{\\lambda}\\). Start from the result of 4.1 and create a function negLoglikelihood which returns the negative loglikelihood; 4.3. The nlm procedure searches for the optimal parameter in the domain \\((-\\infty, \\infty)\\). You will use a transformation \\(\\lambda = \\exp(\\beta)\\) and optimize the likelihood for this parameter \\(\\beta \\in (-\\infty, \\infty)\\). Update the function negloglikelihood to take \\(\\beta\\) as its input; 4.4. Minimize the function negLoglikelihood you defined in 4.3. using the nlm procedure. Add the option hessian = TRUE; 4.5. Interpret the output of nlm. What is the maximum likelihood estimate for \\(\\beta\\) and what about \\(\\lambda\\) (see the discussion in Section 13.3 on Variable and interval estimation). Do you find the same result as in 3.? 4.6. You will now construct a \\(95\\%\\) confidence interval for the unknown parameter \\(\\beta\\) and afterwards for \\(\\lambda\\). Under MLE the actual parameter \\(\\beta\\) is asymptotically distributed as (see Chapter 13 on Variance and interval estimation) \\[ \\beta \\sim \\mathcal{N}(\\hat{\\beta}, \\mathcal{I}^{-1}(\\hat{\\beta})),\\] where \\(\\mathcal{I}\\) denotes the Fisher information matrix. You calculate this matrix as the negative of the Hessian, the matrix with the second order derivatives of the log-likelihood, evaluated in \\(\\hat{\\beta}\\). Of course, since the Exponential distribution only has one paremeter, the matrix reduces to a scalar. 4.6.1. You added the option hessian = TRUE in nlm to obtain the Hessian (numerically) in the nlm procedure. Use the Hessian to calculate the standard error of the MLE \\(\\hat{\\beta}\\). Because you calculated the Hessian of the negative log-likelihood, it suffices to take its inverse to obtain the (asymptotic) variance of the MLE. 4.6.2. A \\(95\\%\\) confidence interval for the actual parameter \\(\\beta\\) is then given by \\[ [\\hat{\\beta} - \\Phi^{-1}(0.975) \\cdot \\text{se}_{\\hat{\\beta}}, \\hat{\\beta} + \\Phi^{-1}(0.975) \\cdot \\text{se}_{\\hat{\\beta}}], \\] where \\(\\Phi\\) is the CDF of the standard normal distributon. Calculate the \\(95\\%\\) confidence interval for the intensity \\(\\beta\\) based on the simulated sample. Is the orignal \\(\\beta = \\log \\lambda\\) (used for simulating the data) contained in this interval? 4.6.3 You will now use the delta method (see Section 13.3 in the book) to construct a confidence interval for the unknown \\(\\lambda\\). The MLE for \\(\\lambda\\) is obtained from the transformation \\(\\hat{\\lambda}=\\exp \\hat{\\beta}\\). The corresponding se is calculated as \\(se_{\\hat{\\lambda}} = (\\exp \\hat{\\beta})^2 \\cdot \\text{se}_{\\hat{\\beta}}\\). Using these ingredients you are now ready to construct the confidence interval for the unknown parameter \\(\\lambda\\). # @1 loglikelihood &lt;- function(lambda) { loglikelihood &lt;- nsim * log(lambda) - sum(lambda * sim) return(loglikelihood) } # @2 negLoglikelihood &lt;- function(lambda) { loglikelihood &lt;- nsim * log(lambda) - sum(lambda * sim) return(-loglikelihood) } # @3 negLoglikelihood &lt;- function(beta) { lambda &lt;- exp(beta) loglikelihood &lt;- nsim * log(lambda) - sum(lambda * sim) return(-loglikelihood) } # @4 fit &lt;- nlm(negLoglikelihood, p = 0, hessian = TRUE) Warning in nlm(negLoglikelihood, p = 0, hessian = TRUE): NA/Inf replaced by maximum positive value fit $minimum [1] 26055 $estimate [1] -1.606 $gradient [1] -7.251e-05 $hessian [,1] [1,] 10001 $code [1] 1 $iterations [1] 8 # @5 lambdaMLE &lt;- exp(fit$estimate) lambdaMLE [1] 0.2008 nsim / sum(sim) [1] 0.2008 # @6 sigma.beta &lt;- sqrt(solve(fit$hessian)) sigma.lambda &lt;- sigma.beta * lambdaMLE^2 c(lambda - qnorm(0.975) * sigma.lambda, lambda + qnorm(0.975) * sigma.lambda) [1] 0.1992 0.2008 2.2 Discrete distributions In this computer lab you will simulate discrete data (e.g. claim counts). You will then explore and fit a statistical model to the simulated data set. 2.2.1 Simulating the data You simulate 10 000 observations from a lognormal distribution (a continuous distribution!). You will then discretize the simulated data by rounding down. Simulate 10 000 observations from a lognormal distribution with density \\[f(x) = \\frac{1}{x \\cdot \\sqrt{2 \\pi}} \\cdot \\exp \\left( -(\\ln(x) + 1.5 \\right)^2).\\] Hint: Check the specification of the lognormal distribution in R, ?rlnorm. Discretize the data by rounding down using the floor function. # Example of the floor function x &lt;- runif(6)*3 rbind(x = x, floor = floor(x)) [,1] [,2] [,3] [,4] [,5] [,6] x 2.576 2.158 0.347 0.4791 2.429 0.6439 floor 2.000 2.000 0.000 0.0000 2.000 0.0000 nsim &lt;- 10000; sim &lt;- exp(rnorm(nsim, mean = -1.5, sd = 1)) # we only observe the data discrete sim &lt;- floor(sim) 2.2.2 Exploratory analysis You just obtained simulated discrete data. You now want to investigate which discrete distributions could be good candidates for modelling the simulated data. Start by calculating the mean and variance of the simulated data. Is the data underdispersed or overdispersed? The variance is larger than the mean of the data. The data is overdispersed. Which of the following three distributions will most likely describe the data in a good way? Binomial Poisson Negative binomial The Negative binomial distribution is the best candidate, since this distribution is overdispersed. Test visually whether the data belongs to the \\((a, b, 0)\\) class, i.e. see whether the relation \\[ k \\cdot \\frac{p_k}{p_{k-1}} = a \\cdot k + b, \\quad k = 1,2, \\ldots\\] holds for the simulated data. 3.1 Compute the left hand side (lhs) of this relation. Use prop.table(table(???)) to get the empirical probability distribution \\(p_k\\); The data is heavy tailed and the lhs can become very large when \\(p_{k-1}\\) is small. You check the relation for \\(k = 1, \\dots, 5\\); Create a vector \\(k\\), \\(p_k\\) and \\(p_{k-1}\\) for \\(k = 1,\\ldots,5\\); Combine these results to obtain the lhs of the equation. 3.2 Use ggplot to construct a graph containg the points \\((k, k \\cdot \\frac{p_k}{p_{k-1}})\\). Your graph should look similar to Load the package ggplot2; Create a new ggplot figure. Add a geom_point graphic using the data points \\((k, lhs)\\); ggplot() + geom_point(aes(???, ???)) Change the color of the points to blue by adding the option col = #99CCFF to geom_point; You can further customize the graph with theme_bw(), xlab, ylab, ggtitle, \\(\\ldots\\). 3.3. Discuss. Is a distribution from the \\((a, b, 0)\\) class a good candidate for this data? mean(sim) [1] 0.0897 var(sim) [1] 0.1555 prob &lt;- prop.table(table(sim)) prob sim 0 1 2 3 4 5 6 8 10 0.9319 0.0538 0.0104 0.0023 0.0010 0.0002 0.0001 0.0002 0.0001 k &lt;- 1:5 pk &lt;- prob[k+1] pkmin1 &lt;- prob[k] lhs &lt;- k * pk / pkmin1 ggplot() + geom_point(aes(k[1:5], lhs[1:5]), color = &quot;#99CCFF&quot;) + theme_bw() + xlab(&quot;k&quot;) + ggtitle(&#39;(a, b, 0)-relation test plot&#39;) Don&#39;t know how to automatically pick scale for object of type table. Defaulting to continuous. 2.2.3 Maximum Likelihood Estimation (MLE) You will now fit two count distributions to the simulated data: Geometric Negative binomial (NB) 2.2.3.1 Geometric For a Geometric distribution with parameter \\(p \\in [0, 1]\\) the probability of observing \\(k\\) events is given by \\[ P(N = k) = (1-p)^{k} \\cdot p.\\] In the Appendix ‘An inventory of discrete distributions’ of the Loss Models book you will find a different parameterization. That is \\[ P(N=k) = \\left(\\frac{\\theta}{1+\\theta}\\right)^k \\cdot \\frac{1}{(1+\\theta)}.\\] If you put \\(p = \\frac{1}{1+\\theta}\\) you can work from the second to the first parametrization. Verify this. Derive an expression for the loglikelihood; The likelihood is given by \\[ L(p) = \\prod_{i=1}^{\\text{nsim}} P(N = x_i) = \\prod_{i=1}^{\\text{nsim}} (1-p)^{x_i} \\cdot p.\\] The loglikelihood is \\[ l(p) = \\sum_{i=1}^{\\text{nsim}} \\left( \\log(1-p) \\cdot x_i + \\log(p) \\right).\\] Implement the negative loglikelihood as a function in R; geom.negLoglikelihood &lt;- function(p) { loglikelihood &lt;- ??? return(-loglikelihood) } The probability \\(p\\) can only take values in \\([0, 1]\\). Change the function geom.negLoglikelihood to take a parameter \\(\\beta \\in (-\\infty, \\infty)\\). Then transform the interval \\((-\\infty, \\infty)\\) to \\([0, 1]\\) using the logit transform \\[ \\text{logit}(p) = \\log\\left( \\frac{p}{1-p} \\right) = \\beta. \\] Inverting this expression, you find (verify this!) \\[ p = \\frac{\\exp(\\beta)}{ 1 + \\exp(\\beta) }.\\] Maximize the likelihood using the nlm procedure in R and interpret the results. geom.negLoglikelihood &lt;- function(beta) { p &lt;- exp(beta) / (1+exp(beta)) loglikelihood &lt;- sum(log(1-p) * sim) + nsim * log(p) return(-loglikelihood) } fit &lt;- nlm(geom.negLoglikelihood, 1) Warning in nlm(geom.negLoglikelihood, 1): NA/Inf replaced by maximum positive value Warning in nlm(geom.negLoglikelihood, 1): NA/Inf replaced by maximum positive value fit $minimum [1] 3099 $estimate [1] 2.411 $gradient [1] -0.000222 $code [1] 1 $iterations [1] 6 geom.p &lt;- exp(fit$estimate) / (1+exp(fit$estimate)) geom.p [1] 0.9177 geom.loglik &lt;- -fit$minimum 2.2.3.2 Negative binomial You will now go from the one parameter geometric distribution to a two parameter discrete distribution, the Negative Binomial. Its pf is specified as follows: \\[Pr(N=k) = \\frac{\\Gamma(a+k)}{\\Gamma(a) k!}\\left(\\frac{\\mu}{\\mu+a}\\right)^{k}\\left(\\frac{a}{\\mu+a}\\right)^{a}.\\] Follow the same steps as with the geometric distribution to fit the NB distribution to the simulated data. The parameters \\(\\mu\\) and \\(a\\) can take values on the positive real line \\([0, \\infty)\\). Choose an appropriate transormation to convert this interval to the whole real line, \\((-\\infty, \\infty)\\). NB.negativeLoglikelihood &lt;- function(beta) { mu &lt;- exp(beta[1]) a &lt;- exp(beta[2]) loglikelihood &lt;- sum(lgamma(a + sim) - lgamma(a) - lfactorial(sim) + sim * log(mu/(mu + a)) + a * log(a / (mu + a))) return(-loglikelihood) } fit &lt;- nlm(NB.negativeLoglikelihood, c(0, 0), hessian=TRUE) Warning in nlm(NB.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value Warning in nlm(NB.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value Warning in nlm(NB.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value fit $minimum [1] 2975 $estimate [1] -2.411 -1.893 $gradient [1] 8.958e-04 2.859e-05 $hessian [,1] [,2] [1,] 562.22093 0.01064 [2,] 0.01064 121.47852 $code [1] 1 $iterations [1] 19 # Store the fitted values nb.mu &lt;- exp(fit$estimate[1]) nb.a &lt;- exp(fit$estimate[2]) c(mu = nb.mu, a = nb.a) mu a 0.0897 0.1506 nb.loglik &lt;- -fit$minimum 2.2.4 Comparing fitted models You will now compare which model best fits the data using AIC as well as some visual inspection tools. 2.2.4.1 AIC Suppose that you have a statistical model calibrated on some data. Let \\(k\\) be the number of estimated parameters in the model. Let \\({\\displaystyle {\\hat {L}}}\\) be the maximum value of the likelihood function for the model. Then the AIC of the investigated model is the following \\[ \\text{AIC} = 2 k - 2 \\ln ( \\hat{L}). \\] Given a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit. For more information see wikipedia. Calculate the AIC for both fitted models. Hint: \\(-\\ln ( \\hat{L})\\) is the minimum reached by the nlm procedure. Which of the two models does AIC prefer? aic &lt;- c(geom = 2 * 1 - 2 * geom.loglik, nb = 2 * 2 - 2 * nb.loglik) # the nb distribution has the lowest AIC print(aic) geom nb 6200 5955 2.2.4.2 Visual inspection Using the fitted parameters you will now simulate new datasets of nsim observations from the Geometric and Negative Binomial distribution. You compare the shapes of the fitted and the original data. Simulate a dataset from the Geometric distribution using the fitted parameters; Simulate a dataset from the Negative binomial distribution using the fitted parameters; You will now create a barplot using geom_barplot. First the data has to be merged into a single data frame containing two columns: x: the simulated values; method: a string, referring to the method used to simulate the data (i.e. observed, geom or nb). x method 0 observed 0 observed 0 geom 1 geom 0 nb 3.1 Create datasets df.observed, df.geom, df.nb with the simulated data in one column and a string referring to the method used in the other column. df.observed &lt;- data.frame(x = ???, method = &#39;observed&#39;) 3.2 Combine these three datasets into a single dataset using rbind. df &lt;- rbind(df.observed, df.geom, df.nb); Create a barplot using geom_bar. ggplot() + geom_bar(aes(???, fill = ???)) + theme_bw() By default geom_bar stacks the bars for the different methods. To show the bars sideways add the option position = position_dodge(). Discuss. Which distribution best mimicks the original data? sim.geom &lt;- rgeom(nsim, geom.p) sim.nb &lt;- rnbinom(nsim, mu = nb.mu, size = nb.a) df.observed &lt;- data.frame(x = sim, method = &#39;observed&#39;) df.geom &lt;- data.frame(x = sim.geom, method = &#39;geom&#39;) df.nb &lt;- data.frame(x = sim.nb, method = &#39;nb&#39;) df &lt;- rbind(df.observed, df.geom, df.nb); ggplot() + geom_bar(aes(df$x, fill=df$method), position = position_dodge()) + theme_bw() "],
["putting-it-all-together-case-study-on-modelling-claim-counts.html", "3 Putting it all together: case-study on modelling claim counts 3.1 Read in data 3.2 Exploratory analysis 3.3 Fitting count distributions 3.4 AIC 3.5 Replicating data sets 3.6 Mean and variance of the estimated ZIP, NB, Hurdle Poisson 3.7 Conclusion", " 3 Putting it all together: case-study on modelling claim counts In this tutorial you will import a data set with the number of claims registered on a group of policyholders during one year. You will look for a suitable discrete distribution to model this data set. Hereto you will fit the Poisson, the Negative Binomial, the Zero-Inflated and the Hurdle Poisson to the data, while estimating the parameters used by these distributions with Maximum Likelihood Estimation. As a final step, you will compare the different model fits and select the best fitting parametric distribution. 3.1 Read in data Importing data is often the first step in any analysis. In this tutorial the data is stored in the file NonFleetCo507Final.txt, which is located in a local subdirectory data. Check out chapter 4 of Data science in insurance: an R intro for a detailed overview of the R methods for importing data. 3.1.1 Determining the file path Before we can import the data, we should first determine the exact file path where the data are located. R offers several methods for this task. file.choose() file.choose() opens an interactive prompt, which allows you to manually select the location of the file. Once selected R prints the aboslute path to the file in the console. file.choose() &quot;C:\\Users\\u0043788\\Dropbox\\Risk modelling course Ljubljana\\Bookdown\\data\\NonFleetCo507Final.txt&quot; # Store the path in a variable for later use. path &lt;- &quot;C:\\\\Users\\\\u0043788\\\\Dropbox\\\\Risk modelling course Ljubljana\\\\Bookdown\\\\data\\\\NonFleetCo507Final.txt&quot; setwd(&lt;path&gt;) setwd(&lt;path&gt;) specifies the working directory of the current R session to &lt;path&gt;. Once set all files in the working directory can be referenced by relative paths. # This is the map containing all files for the tutorial setwd(&quot;C:\\\\Users\\\\u0043788\\\\Dropbox\\\\Risk modelling course Ljubljana\\\\Bookdown\\\\&quot;) # This is a relative path from the working directory to the file we want to import path &lt;- &quot;data\\\\NonFleetCo507Final.txt&quot; Additional methods for RStudio RStudio offers two additional methods for setting the working directory to the location of the current R file. Method 1: In the menu click Session -&gt; Set Working Directory -&gt; To Source File Location. Method 2: Run the following code in the R console dir &lt;- dirname(rstudioapi::getActiveDocumentContext()$path) setwd(dir) path &lt;- &quot;data\\\\NonFleetCo507Final.txt&quot; The advantage of these methods is that the working directory is automatically updated when the file is moved. 3.1.2 Importing a .txt file After obtaining the file path, we read in the txt file using read.table. We specify the following options: header = TRUE: The first row of the file contains the variable names. sep = \\t: A tab splits the records in the text file. NonFleet &lt;- read.table(file = path, header = TRUE, sep = &#39;\\t&#39;) The data is now imported in the data.frame NonFleet. head(&lt;data.frame&gt;) prints the first records of a data.frame. This is a good first check to see whether the data was imported correctly. # Show the first records of the imported data set head(NonFleet) AgeInsured SexInsured Experience TLength Clm_Count VAge PrivateCar NCD_0 1 32 M 11 0.4654 0 10 1 0 2 26 M 5 0.8077 0 13 1 1 3 32 M 5 0.3997 0 0 1 0 4 32 M 5 0.5832 0 1 1 0 5 41 M 14 0.7748 0 9 1 0 6 28 F 3 0.4928 0 0 1 1 Cover_C VehCapCubic VehCapTonn 1 1 1797 0 2 0 1590 0 3 1 1997 0 4 1 1997 0 5 0 1597 0 6 1 1587 0 Everything looks good. In this tutorial we focus on the variables: Clm_Count: Number of claims for the policyholder; TLength: Fraction of the year that the policyholder was insured. In insurance this is often called the exposure. We create separate variables in R to store these covariates Clm_Count &lt;- NonFleet$Clm_Count; TLength &lt;- NonFleet$TLength; 3.2 Exploratory analysis We now explore the available data and analyze the number of claim counts per insured. 3.2.1 Summary statistics disregarding exposure We start our analysis by computing the mean and variance of the number of observed claims. If we denote by \\(n_i\\) the number of claims observed for policyholder \\(i\\), we can compute the mean and variance as \\[ \\mu = E(X) = \\frac{1}{m} \\cdot \\sum_{i=1}^m n_i\\] and \\[ \\sigma^2 = E((X - \\mu)^2) = \\frac{1}{m} \\cdot \\sum_{i=1}^m (n_i - \\mu)^2. \\] In these formulas \\(m\\) denotes the number of observations. m &lt;- length(Clm_Count) mu &lt;- sum(Clm_Count) / m var &lt;- sum((Clm_Count - mu)^2) / m c(mean = mu, variance = var) mean variance 0.09848 0.10925 3.2.2 Summary statistics taking into account exposure The previous calculation of the mean and variance does not consider the difference in exposure between policyholders. However, it is important to take exposure into account. Let \\(d_i\\) be the exposure for policyholder \\(i\\), then we calculate the mean as \\[ \\mu_{\\text{exp}} = \\sum_{i=1}^m \\frac{d_i}{\\sum_{i=1}^m d_i} \\frac{n_i}{d_i} = \\frac{\\sum_{i=1}^m n_i}{\\sum_{i=1}^m d_i}\\] and the variance as \\[ \\sigma^2_{\\text{exp}}=\\frac{\\sum_{i=1}^m(n_i-\\mu_{exp} \\cdot d_i)^2}{\\sum_{i=1}^m d_i} . \\] For more intuition behind these estimators, check out the blog of Arthur Charpentier and Section 15.6.6 from Klugman et al.. mu &lt;- sum(Clm_Count) / sum(TLength); var &lt;- sum((Clm_Count-mu*TLength)^2)/sum(TLength) c(mean = mu, variance = var) mean variance 0.1546 0.1675 This is the expected number of accidents for a policyholder who is insurerd throughout the whole year, i.e. \\(d_i = 1\\). 3.2.3 Empirical probability distribution table allows us to easily construct a contingency table of the counts. table(Clm_Count) Clm_Count 0 1 2 3 4 5 145683 12910 1234 107 12 1 R can plot this table plot(table(Clm_Count)) prop.table can be used to obtain the empirical probability distribution prop.table(table(Clm_Count)) Clm_Count 0 1 2 3 4 5 9.108e-01 8.071e-02 7.715e-03 6.690e-04 7.502e-05 6.252e-06 We can create a better barplot using ggplot ggplot(): starts the construction of a ggplot figure; geom_bar(...): creates a bar plot; aes(&lt;var&gt;): specifies the variables used to create the plot. # Run the following line of code when the package ggplot2 is not yet installed. # install.package(ggplot2) # Load the package ggplot2 library(ggplot2) ggplot() + geom_bar(aes(Clm_Count)) To specify your own theme, you define some visualisation parameters and colors that will be used in your ggplot calls. col &lt;- &quot;#003366&quot; fill &lt;- &quot;#99CCFF&quot; Instead of manually changing all details of the plot, ggplot also offers some general layout schemes. In this tutorial we use the black and white theme theme_bw(). ggplot() + geom_bar(aes(Clm_Count), col = col, fill = fill) + theme_bw() The weight argument in aes allows you to weight the number of policyholders who file 0 claims, 1 claim and so on by exposure instead of simply counting the number of policyholders. ggplot() + geom_bar(aes(Clm_Count, weight = TLength), col = col, fill = fill) + theme_bw() You should check ggplot2 barplot to learn more. https://ggplot2.tidyverse.org/reference/geom_bar.html 3.2.4 The (a, b, 0) class of distributions We test whether the data could come from a distribution in the (a, b, 0) class of distributions. Distributions in this family satisfy \\[ \\frac{k \\cdot p_k}{p_{k-1}} = a \\cdot k+ b, \\quad k = 1,\\ldots,\\infty \\] geom_point: adds a scatterplot to ggplot. Two variables have to be specified in aes. xlab: specifies the name of the label on the x-axis. # We first determine the empirical probabilities p_k p &lt;- as.numeric(table(Clm_Count) / length(Clm_Count)) # We calculate the left hand side (lhs) of the relation above lhs &lt;- (1:(length(p)-1)) * p[2:length(p)] / p[1:(length(p)-1)] ggplot() + geom_point(aes(x = 1:(length(p)-1), y = lhs)) + xlab(&#39;k&#39;) + ylab(&#39;lhs&#39;) + theme_bw() You should check ggplot2 geom_point to learn more. https://ggplot2.tidyverse.org/reference/geom_point.html The observations \\((k, \\frac{k \\cdot p_k}{p_{k-1}})\\) seem to be on a straight line with positive intercept. This indicates that the Negative Binomial distribution might be a good fit for the data. 3.3 Fitting count distributions We fit several count distributions to the observed claim count data: Poisson Negative binomial (NB) Modified Poisson distributions We do not consider the explanatory variables, but take the exposure into account. We fit these distributions using Maximum Likelihood Estimation (MLE). 3.3.1 Poisson For a Poisson distribution with intensity \\(\\lambda\\) the probability of observing \\(k\\) events is given by \\[ P(N = k) = \\exp(-\\lambda) \\frac{\\lambda^k}{k!}.\\] The expected value of the poisson distribution is \\[ E(N) = \\lambda. \\] Not all policyholders are insured throughout the whole year (\\(d_i = 1\\)) and obviously policyholders who are only at risk for a small fraction of the year are less likely to experience a claim. We assume that the claim intensity is proportional to the exposure, i.e. \\[ N_i \\sim \\text{POI}(d_i \\cdot \\lambda),\\] such that the expected value scales with exposure \\[ E(N_i) = d_i \\cdot \\lambda.\\] We then interpret \\(\\lambda\\) as the expected number of claims for a policyholder who was insured throughout the whole year. Let \\(m\\) be the number of observations and \\(n_i\\) be the observed number of claims for policyholder \\(i\\), then the likelihood is given by \\[ \\mathcal{L}(\\lambda) = \\prod_{i=1}^m P(N_i = n_i) = \\prod_{i=1}^m \\exp(-\\lambda \\cdot d_i) \\cdot \\frac{(\\lambda \\cdot d_i)^{n_i}}{n_i!}\\] and the loglikelihood is \\[ \\ell(\\lambda) = \\sum_{i=1}^m -\\lambda \\cdot d_i + n_i \\cdot \\log(\\lambda \\cdot d_i) - \\log(n_i !).\\] We want to maximize this loglikelihood with respect to \\(\\lambda\\). We define a function poisson.loglikelihood which returns the loglikelihood as a function of \\(\\lambda\\). poisson.loglikelihood &lt;- function(lambda) { loglikelihood &lt;- sum(-lambda * TLength + Clm_Count * log(lambda * TLength) - lfactorial(Clm_Count)) return(loglikelihood) } Unfortunately it is not possible to maximize this function directly in R. We make two small adjustments We will use the nlm (non-linear minimizer) function in R for finding the maximum likelihood parameter \\(\\hat{\\lambda}\\). Because, nlm is used to minimize a function, we change the routine to return the negative loglikelihood (\\(-\\ell(\\lambda)\\)). Minmizing the negative loglikelihood is equivalent to maximizing the loglikelihood. The parameter \\(\\lambda\\) is restricted to positive values. The built-in algorithms in R look for parameters in the unrestricted domain \\((-\\infty, \\infty)\\). We reparametrize the likelihood and optimize for \\(\\beta = \\log(\\lambda)\\), which can take values in \\((-\\infty, \\infty)\\). poisson.negLoglikelihood &lt;- function(beta) { lambda = exp(beta) return(-poisson.loglikelihood(lambda)) } We use the non-linear minimization function nlm to carry out the minimization. This routine requires a starting value for \\(\\beta\\), which is here simply set to 0. The function returns a list containing the following output (check the help page ?nlm for more details): minimum: the value of the estimated minimum of f. estimate: the point at which the minimum value of f is obtained. gradient: the gradient (first derivative) at the estimated minimum of f. The gradient should be close to zero. hessian: the hessian (second derivative) at the estimated minimum of f. The hessian is used to determine confidence bounds for the parameters fitted through maximum likelihood. code: an integer indicating why the optimization process terminated. When code equals 1 or 2, the algorithm converged and the current estimate is probably the solution. When code equals 3, 4 or 5 the algorithm has not converged. For more details see the help page of nlm (?nlm). fit &lt;- nlm(poisson.negLoglikelihood, 0, hessian = TRUE) Warning in nlm(poisson.negLoglikelihood, 0, hessian = TRUE): NA/Inf replaced by maximum positive value fit $minimum [1] 50834 $estimate [1] -1.867 $gradient [1] 0.0003585 $hessian [,1] [1,] 15754 $code [1] 1 $iterations [1] 7 # Store the fitted lambda value poisson.lambda &lt;- exp(fit$estimate) # Store the minimal value found for the loglikelihood poisson.loglik &lt;- poisson.loglikelihood(poisson.lambda) The estmiate for \\(\\lambda\\) is identical to the expected value \\(\\mu_{\\text{exp}}\\) we calculated earlier by taking exposure into account. When we fit the data with maximum likelihood we obtain a point estimate \\(\\hat{\\lambda}\\) for the intensity of the Poisson process However, sometimes (e.g. when doing hypothesis testing) we also need a confidence interval for the actual parameter \\(\\lambda\\). Under MLE we know that the actual parameters of the distribution are assymptoticaly distributed as (see Chapter 13 on Variance and interval estimation) \\[ \\lambda \\sim \\mathcal{N}(\\hat{\\lambda}, \\mathcal{I}^{-1}),\\] where \\(\\mathcal{I}\\) denotes the Fisher information matrix. You calculate this matrix as the negative of the Hessian, the matrix with the second order derivatives of the log-likelihood, evaluated in \\(\\hat{\\beta}\\). # The standard error of the parameter estimate for beta is beta.se = sqrt(solve(fit$hessian)) You can now use the delta method (see Section 13.3 in the book) to construct a confidence interval for the unknown \\(\\lambda\\). The MLE for \\(\\lambda\\) is obtained from the transformation \\(\\hat{\\lambda}=\\exp \\hat{\\beta}\\). The corresponding se is calculated as \\(se_{\\hat{\\lambda}} = (\\exp \\hat{\\beta})^2 \\cdot \\text{se}_{\\hat{\\beta}}\\). We can use this normal approximation to calculate a \\(95\\%\\) confidence interval for the actual parameter \\(\\lambda\\) as \\[ [\\lambda - \\Phi^{-1}(0.975) \\cdot \\sigma_{\\lambda}, \\lambda + \\Phi^{-1}(0.975) \\cdot \\sigma_{\\lambda}] \\], where \\(\\Phi\\) is the CDF of the standard normal distributon and \\(\\sigma_{\\lambda}\\) is the standard error of the \\(\\lambda\\) parameter. # The standard error of the parameter estimate for lambda is poisson.se = poisson.lambda^2 * beta.se; # The 95% confidence interval for lambda is c(poisson.lambda - qnorm(0.975) * poisson.se, poisson.lambda + qnorm(0.975) * poisson.se) [1] 0.1542 0.1549 3.3.2 Negative binomial The probability function for the negative binomial distribution is given by \\[Pr(N=k) = \\frac{\\Gamma(a+k)}{\\Gamma(a) k!}\\left(\\frac{\\mu}{\\mu+a}\\right)^{k}\\left(\\frac{a}{\\mu+a}\\right)^{a}.\\] We take exposure into account and model \\(\\mu_i = d_i \\cdot \\mu\\), where \\(\\mu\\) is the expected number of claims for a policyholder who is insured for a full year. The likelihood now contains two parameters \\(a\\) and \\(\\mu\\) which we have to optimize simultaneously. We define the negative loglikelihood NB.negativeLoglikelihood &lt;- function(beta) { mu &lt;- exp(beta[1]) a &lt;- exp(beta[2]) loglikelihood &lt;- sum(lgamma(a + Clm_Count) - lgamma(a) - lfactorial(Clm_Count) + Clm_Count * log(mu*TLength/(mu*TLength + a)) + a * log(a / (mu * TLength + a))) return(-loglikelihood) } In this case it’s more important to supply good starting values for the convergence speed of the algorithm. For the Negative Binomial distribution \\[ E(X) = \\mu \\quad \\text{and} \\quad Var(X) = \\mu + \\frac{1}{a} \\cdot \\mu^2.\\] We match the first two moments and set \\[ \\mu = E(X) \\quad \\text{and} \\quad a = \\frac{\\mu^2}{Var(X) - \\mu}.\\] mu.initial &lt;- mu a.initial &lt;- mu^2 / (var - mu) fit &lt;- nlm(NB.negativeLoglikelihood, log(c(mu.initial, a.initial)),hessian=TRUE) # Store the fitted values nb.mu &lt;- exp(fit$estimate[1]) nb.a &lt;- exp(fit$estimate[2]) # Store the minimal value found for the loglikelihood nb.loglik &lt;- -fit$minimum 3.3.3 Modified Poisson distributions We consider two popular modifications of the Poisosn distribution, namely the Zero Inflated Poisson (ZIP) distribution and the Hurdle Poisson distribution. 3.3.3.1 Zero Inflated Poisson (ZIP) The ZIP distribution is a Poisson distribution where the probability of having zero claims is increased by \\(p\\). \\[ P(N = k) = \\begin{cases} p + (1-p) \\cdot P(\\tilde{N} = 0) &amp; k = 0 \\\\ (1-p) \\cdot P(\\tilde{N} = k) &amp; k &gt; 0 \\end{cases},\\] where \\(\\tilde{N}\\) follows a Poisson distribution. The ZIP distribution is the mixture of a degnerate distribution in zero with weight \\(p\\) and a Poisson distribution with weight \\(1-p\\). The parameter \\(p\\) can take values in \\([0, 1]\\), we transform the interval \\([0, 1]\\) to the real line \\((-\\infty, \\infty)\\) using the logit transform \\[ \\text{logit}(p) = \\log\\left( \\frac{p}{1-p} \\right) = \\beta. \\] Inverting this expression, we find \\[ p = \\frac{\\exp(\\beta)}{ 1 + \\exp(\\beta) }.\\] ZIP.negativeLoglikelihood &lt;- function(beta) { lambda &lt;- exp(beta[1]) p &lt;- exp(beta[2])/(1+exp(beta[2])) density &lt;- (p + (1-p) * exp(-TLength * lambda))^(Clm_Count == 0) * ((1-p) * exp(-TLength * lambda) * (TLength *lambda)^Clm_Count / gamma(Clm_Count+1))^(Clm_Count != 0) loglikelihood &lt;- sum(log(density)) return(-loglikelihood) } fit &lt;- nlm(ZIP.negativeLoglikelihood, c(0, 0),hessian=TRUE) Warning in nlm(ZIP.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value Warning in nlm(ZIP.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value fit $minimum [1] 50663 $estimate [1] -1.3754 -0.4551 $gradient [1] -0.02996 0.02012 $hessian [,1] [,2] [1,] 14607 -5623 [2,] -5623 2423 $code [1] 1 $iterations [1] 13 ZIP.lambda &lt;- exp(fit$estimate[1]) ZIP.p &lt;- exp(fit$estimate[2])/(1+exp(fit$estimate[2])) c(lambda = ZIP.lambda, p = ZIP.p) lambda p 0.2527 0.3881 ZIP.loglik &lt;- -fit$minimum Many policyholders file zero claims, which is captured by increasing the probability of observing zero claims by \\(38.8\\%\\). 3.3.3.2 Hurdle Poisson In the Hurdle Poisson we set the probability of observing zero claims to \\(p\\). Conditional on there being a claim the distribution follows a zero-truncated Poisson distribution. The probability of observing \\(k\\) claims becomes \\[P(N = k) = \\begin{cases} p &amp; k = 0 \\\\ (1-p) \\cdot P(\\tilde{N} = k \\mid \\tilde{N} &gt; 0) &amp; k &gt; 0 \\end{cases},\\] where \\(\\tilde{N}\\) follows a Poisson distribution. The probability distribution of the zero-truncated Poisson distribution is given by \\[ P(\\tilde{N} = k \\mid \\tilde{N} &gt; 0) = \\frac{P(\\tilde{N} = k)}{P(\\tilde{N} &gt; 0)} = \\frac{P(\\tilde{N} = k)}{1- \\exp(-\\lambda)}.\\] We assume that the intensity of the zero-truncated Poisson distribution is proportional to the exposure, i.e. \\(\\lambda_i = d_i \\cdot \\lambda\\). The probability of observing zero claims is \\(p\\) and does not depend on the exposure \\(d_i\\). Hurdle.negativeLoglikelihood &lt;- function(beta) { lambda &lt;- exp(beta[1]) p &lt;- exp(beta[2])/(1+exp(beta[2])) density &lt;- (p)^(Clm_Count == 0) * ((1-p) * exp(-TLength * lambda) / (1-exp(-lambda * TLength)) * (TLength *lambda)^Clm_Count / gamma(Clm_Count+1))^(Clm_Count != 0) loglikelihood &lt;- sum(log(density)) return(-loglikelihood) } fit &lt;- nlm(Hurdle.negativeLoglikelihood, c(0, 0),hessian=TRUE) Warning in nlm(Hurdle.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value Warning in nlm(Hurdle.negativeLoglikelihood, c(0, 0), hessian = TRUE): NA/Inf replaced by maximum positive value fit $minimum [1] 52955 $estimate [1] -1.381 2.324 $gradient [1] 4.741e-05 1.088e-02 $hessian [,1] [,2] [1,] 1.541e+03 -3.131e-04 [2,] -3.131e-04 1.299e+04 $code [1] 1 $iterations [1] 12 Hurdle.lambda &lt;- exp(fit$estimate[1]) Hurdle.p &lt;- exp(fit$estimate[2])/(1+exp(fit$estimate[2])) c(lambda = Hurdle.lambda, p = Hurdle.p) lambda p 0.2513 0.9108 Hurdle.loglik &lt;- -fit$minimum 3.4 AIC Suppose that we have a statistical model of some data. Let k be the number of estimated parameters in the model. Let \\({\\displaystyle {\\hat {L}}}\\) be the maximum value of the likelihood function for the model. Then the AIC value of the model is the following \\[ \\text{AIC} = 2 k - 2 \\ln ( \\hat{\\mathcal{L}}) \\] Given a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit. For more information see wikipedia. AIC &lt;- round(c(&quot;AIC Poi&quot; = -2 * poisson.loglik + 2 * 1, &quot;AIC NB&quot; = - 2 * nb.loglik + 2 * 2, &quot;AIC ZIP&quot; = -2 * ZIP.loglik + 2 * 2, &quot;AIC Hurdle&quot; = -2 * Hurdle.loglik + 2 * 2)) AIC AIC Poi AIC NB AIC ZIP AIC Hurdle 101670 101318 101330 105914 AIC[which.min(AIC)] AIC NB 101318 The lowest AIC value is achieved by the NB distribution, closely followed by the ZIP. The Hurdle distribution attains a remarkably higher AIC value which reflects the poor way in which the exposure is incorporated. 3.5 Replicating data sets We now show how to generate replicating data sets based on each of these models. We generate 5 random samples of the same size as the original data set and using the estimated parameters. We then plot the contingency tables to compare. 3.5.1 Poisson rpois allows us to generate from a Poisson distribution for any given positive lambda. We use the fitted lambdas, taking exposure into account. Notice that the length of argument lambda is \\(n\\) whereas we sample \\(5n\\) observations. rpois deals with this mismatch by recycling the lambdas \\(5\\) times. The resulting output vector is then used to form a matrix with \\(5\\) columns, filled by its columns (by default), such that each column contains a replicating data set. n &lt;- length(Clm_Count) Repl.Poi &lt;- matrix(rpois(n*5, lambda = poisson.lambda*TLength), c(n,5)) # or fitted(fm_pois) instead of Poi.lambda*TLength par(mfrow=c(2,3)) plot(table(Clm_Count), col=&quot;red&quot;) for(i in 1:5){ plot(table(Repl.Poi[,i]), xlim=c(0,5)) } 3.5.2 NB rnbinom allows us to generate from a Negative Binomial distribution for a given mu and size. The rest is similar to the Poisson case. Repl.NB &lt;- matrix(rnbinom(n*5, mu = nb.mu * TLength, size = nb.a), c(n,5)) par(mfrow=c(2,3)) plot(table(Clm_Count),col=&quot;red&quot;) for(i in 1:5){ plot(table(Repl.NB[,i]), xlim=c(0,5)) } 3.5.3 ZIP To generate data from the ZIP distribution we first simulate Poisson distributed data. Afterwards, each observation is set to zero with probability \\(p\\). # install.packages(&#39;VGAM&#39;) # install.packages(&#39;gamlss.dist&#39;) Repl.ZIP &lt;- matrix(rpois(n * 5, lambda = ZIP.lambda * TLength) * (runif(n * 5) &gt; ZIP.p), c(n, 5)) par(mfrow=c(2,3)) plot(table(Clm_Count),col=&quot;red&quot;) for(i in 1:5){ plot(table(Repl.ZIP[,i]), xlim=c(0,5)) } 3.5.4 Hurdle Poisson We first simulate data from a Poisson distribution, where we discard all observations with zero claims. As such, this is a simulation from a zero-truncated distribution. Afterwards, each observation is set to zero with probability \\(p\\). initial.sim &lt;- rpois(n * 50, lambda = Hurdle.lambda * TLength) initial.sim &lt;- initial.sim[initial.sim &gt; 0] Repl.Hurdle &lt;- matrix(initial.sim[1:(n*5)] * (runif(n * 5) &gt; Hurdle.p), c(n, 5)) par(mfrow=c(2,3)) plot(table(Clm_Count),col=&quot;red&quot;) for(i in 1:5){ plot(table(Repl.Hurdle[,i]), xlim=c(0,5)) } 3.6 Mean and variance of the estimated ZIP, NB, Hurdle Poisson We calculate the mean and variance of the estimated Poisson, NB, ZIP, and Hurdle Poisson (with exposure equal to one) and compare these with the empirical mean and variance. We expect that a model fitting the data well will have a similar mean and variance. Try to derive the stated expressions for the mean and variance yourself as an exercise. 3.6.1 Poisson For the Poisson distribution \\(N \\sim Poi(\\lambda)\\), the mean and the variance are both equal to \\(\\lambda\\). This is called equidispersion. poisson.lambda [1] 0.1546 3.6.2 NB For the negative binomial distribution \\(N \\sim NB(a, \\lambda)\\), the mean equals \\[ E(N) = \\lambda \\] and the variance \\[ \\mathrm{var}(N) = \\lambda + \\lambda^2 / a \\] exceeds the mean (overdispersion). NB.mean &lt;- nb.mu NB.mean [1] 0.1546 NB.var &lt;- nb.mu + nb.mu^2 / nb.a NB.var [1] 0.171 3.6.3 ZIP For the zero inflated Poisson distribution \\(N \\sim ZIP(p, \\lambda)\\), the mean equals \\[ E(N) = (1-p)\\lambda \\] and the variance \\[ \\mathrm{var}(N) = (1-p)(\\lambda^2 + \\lambda ) - (1-p)^2 \\lambda ^2 = E(N) + \\frac{p}{1-p} E(N)^2. \\] From the last expression we notice that the ZIP is overdispersed. ZIP.mean &lt;- (1-ZIP.p)*ZIP.lambda ZIP.mean [1] 0.1546 ZIP.var &lt;- (1-ZIP.p)*(ZIP.lambda^2+ZIP.lambda) - (1-ZIP.p)^2*ZIP.lambda^2 ZIP.var &lt;- ZIP.mean + ZIP.p/(1-ZIP.p)*ZIP.mean^2 ZIP.var [1] 0.1698 3.6.4 Hurdle Poisson For the hurdle Poisson distribution \\(N \\sim \\mathrm{Hurdle}(p, \\lambda)\\), the mean equals \\[ E(N) = \\frac{1-p}{1-e^{-\\lambda}}\\lambda \\] and the variance \\[ \\mathrm{var}(N) = \\frac{1-p}{1-e^{-\\lambda}} (\\lambda^2 + \\lambda) - E(N)^2 = E(N) + \\frac{p-e^{-\\lambda}}{1-p} E(N)^2. \\] From the last expression we notice that the Hurdle Poisson is overdispersed if \\(p &gt; e^{-\\lambda}\\), or, if the probability mass at zero is larger than it would be under a regular Poisson setting. # note that Hurdle.p = 1-fhurdle.p = q = P(N=0) Hurdle.mean &lt;- (1-Hurdle.p)/(1-exp(-Hurdle.lambda))*Hurdle.lambda Hurdle.mean [1] 0.1009 Hurdle.var &lt;- (1-Hurdle.p)/(1-exp(-Hurdle.lambda))*(Hurdle.lambda^2+Hurdle.lambda) - Hurdle.mean^2 Hurdle.var &lt;- Hurdle.mean + (Hurdle.p-exp(-Hurdle.lambda))/(1-Hurdle.p)*Hurdle.mean^2 Hurdle.var [1] 0.116 3.6.5 Comparison with empirical mean and variance means &lt;- c(&quot;mean Obs&quot; = mu, &quot;mean Poisson&quot; = poisson.lambda, &quot;mean NB&quot; = NB.mean, &quot;mean ZIP&quot; = ZIP.mean, &quot;mean Hurdle&quot; = Hurdle.mean) means mean Obs mean Poisson mean NB mean ZIP mean Hurdle 0.1546 0.1546 0.1546 0.1546 0.1009 All distributions expect the hurdle distribution closely approximate the mean of the data. variances &lt;- c(&quot;variance Obs&quot; = var, &quot;variance Poisson&quot; = poisson.lambda, &quot;variance NB&quot; = NB.var, &quot;variance ZIP&quot; = ZIP.var, &quot;variance Hurdle&quot;=Hurdle.var) variances variance Obs variance Poisson variance NB variance ZIP 0.1675 0.1546 0.1710 0.1698 variance Hurdle 0.1160 The hurlde distribution also severly underestimates the variance in the data. Since the NB distribution and the ZIP have two parameters they are more flexibility and can capture both the mean and variance well. 3.7 Conclusion We have investigated the number of claims per policyholder when taking exposure into account. All of our analyses show that the NB distribution is a good fit. The NB has the lowest AIC, the mean and variance of the data are well captured and relation of the (a, b, 0) class \\[ \\frac{k \\cdot p_k}{p_{k-1}} = a \\cdot k + b, \\] seems to be satisfied for a positive value of \\(a\\). "],
["simulation.html", "4 Simulation 4.1 Severity 4.2 Aggregate loss 4.3 Simulating future life times of newborns", " 4 Simulation You will now focus on simulation methods. You will first learn how to draw simulations from a specified, parametric distribution using the Probability Integral Transform (PIT) technique. Then, you will apply your insights to a real-life example where the future lifetimes of newborns are simulated using a recent life table published by Statistics Belgium. 4.1 Severity In this exercise you will simulate \\(100\\) losses \\(X\\) from a Pareto distribution with distribution function \\[ F_X(x) = 1 - \\left( \\frac{1000}{1000 + x}\\right)^3.\\] You will use these simulated observations to calculate the expected loss with a deductible of \\(300\\), i.e. \\(E[(X - 300)_+]\\). 4.1.1 Probability integral transform The probability integral transform (PIT) refers to the following property (see Section 20.1 in the Loss Models book) If a random variable \\(X\\) has a continuous distribution with CDF \\(F_X\\). Then the random variable \\(Y\\) defined as \\(Y = F_X(X)\\) has a uniform distribution. A modified version of this property is often used when simulating data Given a CDF \\(F_X\\) of a continuous distribution and a uniformly distributed random variable \\(U\\) on \\([0, 1]\\). Then \\(F_X\\) is the CDF of the random variable \\(X = F_X^{-1}(U)\\). For more information see probability integral transform and inverse transform sampling. Because of this property, you can follow the steps below to simulate \\(n\\) independent observations from a distribution with CDF \\(F_X\\): Simulate \\(n\\) indepdendent observations \\(u_1, \\ldots, u_n\\) from a uniform distribution (runif in R) on \\([0, 1]\\); Calculate \\(F_X^{-1}\\); \\(F_X^{-1}(u_1), \\ldots, F_X^{-1}(u_n)\\) are \\(n\\) independent observations with CDF \\(F_X\\). Now follow these steps in R and generate \\(100\\) independent observations from a Pareto distribution. Create a vector u containing \\(100\\) independent observations from a uniform distribution; Compute \\(F_X^{-1}\\) on paper and implement \\(F_X^{-1}\\) as a function in R; Calculate \\(F_X^{-1}(u)\\). # @1. n &lt;- 100; u &lt;- runif(n, 0, 1) # @2. cdf.inverse &lt;- function(y) { return(1000 / (1-y)^{1/3} - 1000) } # @3. sim &lt;- cdf.inverse(u) 4.1.2 Visualization You will create the following visualization to compare the actual and empirical CDF. Create a function cdf in R with input a vector \\(x\\) and output a vector \\(F_X(x)\\); Compute the theoretical cdf in the simulated points and store the result in a vector \\(y\\); Load the package ggplot; Complete the following ggplot command to plot the empiciral and theoretical CDF. col = &quot;empirical CDF&quot; selects a different color for each line and adds a legend to the figure; scale_colour_manual specifies the colors to be used in in the figure. The first argument specifies the title for the legend; ggplot() + stat_ecdf(aes(???, col = &quot;empirical CDF&quot;)) + geom_line(aes(???, ???, col = &quot;theoretical CDF&quot;)) + scale_colour_manual(&quot;legend title&quot;, values=c(&quot;#99CCFF&quot;,&quot;#e58920&quot;)) theme(...) changes many visual aspects of a ggplot figure. Search in the documentation for an option to move the legend to the bottom of the figure. Improve your graph by adding options such as theme_bw(), xlab, ylab, ggtitle, …. 4.1.3 Expected loss with a deductible Statistical quantities of the distribution can be approximated by means of the simulated data. You will now calculate the expected loss when there is a deductible of \\(300\\) using your simulated data. Write a function deductible taking as input x (a vector containing the loss amounts) and d (the deductible) and returning a vector \\(y\\) with \\(y_i = (x_i-d)_+ = \\max(x_i-d, 0)\\); Test the function deductible that you defined in the previous step. What is the output for dedutible(c(500, 200), 300). Is your function working as expected? A common mistake in implementing the deductible function is a misunderstanding of the max function in R; # returns the maximum of 1, 3 and 2 max(c(1, 3), 2) [1] 3 # returns a vector containing max(1, 2) and max(3, 2) pmax(c(1, 3), 2) [1] 2 3 Calculate \\(E(X-300)_+\\) for your simulated vector. deductible &lt;- function(x, d) { return(pmax(x-d, 0)) } mean(deductible(sim, 300)) [1] 284.1 4.2 Aggregate loss The number of claims \\(N\\) on an insurance contract is Poisson distributed with mean \\(2\\). The claim sizes \\(X\\) are independently distributed with CDF \\[ F_X(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\ x^2 &amp; 0 &lt; x \\leq 1 \\\\ 1 &amp; x &gt; 1 \\end{cases}. \\] The insurer wants to investigate the difference between imposing an aggregate or an ordinary deductible for this contract. You will calculate (using 10000 simulations) the expected loss by contract for both an aggregate deductible of \\(0.2\\) and an ordinary deductible of \\(0.2\\). Simulate \\(10000\\) observations for the number of claims \\(N\\). (Hint rpois); Use what you learnt in the previous exercise and create a function simulate, which returns n simulations of the claim size distribution \\(F_X\\). simulate &lt;- function(n) { ?? } Complete the following for-loop to simulate 10000 aggregate losses when there is no deductible n &lt;- 10000 nclaim &lt;- # Result from 1. # create an empty vector to store the aggregated loss aggregated.loss &lt;- rep(0, n) for(i in 1:n) { # the claims for the i-th contract claims &lt;- simulate(nclaim[i]) # without deductible the aggregated loss is the sum of the individual claims aggregated.loss[i] &lt;- sum(claims) } # Calculate the expected loss per policy, when there is no deductible mean(aggregated.loss) Adapt the code in 3. to calculate the expected loss per policy when there is an aggregate deductible of \\(0.2\\); Adapt the code in 3. to calculate the expected loss per policy when there is an ordinary deductible of \\(0.2\\). # @1 n &lt;- 10000 nclaim &lt;- rpois(n, lambda = 2) # @2 cdf.inverse &lt;- function(x) { return(sqrt(x)) } simulate &lt;- function(n) { u &lt;- runif(n) return(cdf.inverse(u)); } # @3 n &lt;- 10000 nclaim &lt;- rpois(n, lambda = 2) # create an empty vector to store the aggregated loss aggregated.loss &lt;- rep(0, n) for(i in 1:n) { # the claims for the i-th contract claims &lt;- simulate(nclaim[i]) # without deductible the aggregated loss is the sum of the individual claims aggregated.loss[i] &lt;- sum(claims) } # Calculate the expected loss per policy, when there is no deductible mean(aggregated.loss) [1] 1.337 # @4 n &lt;- 10000 nclaim &lt;- rpois(n, lambda = 2) # create an empty vector to store the aggregated loss aggregated.loss &lt;- rep(0, n) for(i in 1:n) { # the claims for the i-th contract claims &lt;- simulate(nclaim[i]) # without deductible the aggregated loss is the sum of the individual claims aggregated.loss[i] &lt;- sum(claims) } # Calculate the expected loss per policy, when there is no deductible mean(pmax(aggregated.loss - 0.2, 0)) [1] 1.153 # @5 n &lt;- 10000 nclaim &lt;- rpois(n, lambda = 2) # create an empty vector to store the aggregated loss aggregated.loss &lt;- rep(0, n) for(i in 1:n) { # the claims for the i-th contract claims &lt;- pmax(simulate(nclaim[i]) - 0.2, 0) # without deductible the aggregated loss is the sum of the individual claims aggregated.loss[i] &lt;- sum(claims) } # Calculate the expected loss per policy, when there is no deductible mean(aggregated.loss) [1] 0.9368 4.3 Simulating future life times of newborns In this exercise you simulate the age at death for 10000 newborns based on the lifetable for Belgian males in 2017 published by stabel. You will use these simulations to construct a histogram as well as a density plot of the lifetime distribution. 4.3.1 Importing the data Download and save the file lifetableMaleBE2017.csv on your local drive. This file contains the columns age and q, where \\[ q_{age} = P(T_0 \\in (age, age + 1] \\mid T_0 &gt; age),\\] with \\(T_0\\) the random variable describing the lifetime of a newborn. Hence, \\(q_{age}\\) is the one year probability of dying at age \\(\\text{age}\\). Save the file lifetableMaleBE2017.csv on your local drive; Use read.table to load the data into R. Semicolons ; separate the data and the first line of the data contains the variable names; Create variables age and q which store the corresponding variables from the dataset. setwd(&#39;C:/Users/u0043788/Dropbox/Risk modelling course Ljubljana/Bookdown&#39;) data &lt;- read.table(&#39;data/lifetableMaleBE2017.csv&#39;, sep = &#39;;&#39;, header = TRUE) age &lt;- data$age; q &lt;- data$q; 4.3.2 Simulate the whole life time First, you simulate the whole remaining lifetime \\(K_0 = \\lfloor{T_0} \\rfloor\\) for a newborn. The whole lifetime is the integer age at which the newborn dies. You calculate the cdf of \\(K_0\\) at integer ages \\(x\\) as follows \\[\\begin{align*} P(K_0 \\leq x) &amp;= P(T_0 &lt; x+1) \\\\ &amp;= 1 - P(T_0 \\geq x+1) \\\\ &amp;= 1 - P(T_0 \\geq x + 1 \\mid T_0 &gt; x) \\cdot P(T_0 &gt; x) \\\\ &amp;= 1 - (1-q_{x}) \\cdot P(T_0 &gt; x) \\\\ &amp;= 1 - \\prod_{i=0}^{x} (1-q_{x-i}). \\end{align*}\\] or you reason as follows \\[\\begin{align*} P(K_0 \\leq x) &amp;= P(K_0=0)+P(K_0=1)+ \\ldots +P(K_0=x) \\\\ &amp;= q_0+(1-q_0)\\cdot q_1 + (1-q_0)\\cdot(1-q_1)\\cdot q_2 + \\ldots + (1-q_0)\\cdot(1-q_1)\\ldots (1-q_{x-1})\\cdot q_x \\\\ &amp;= 1 - \\prod_{i=0}^{x} (1-q_{x-i}). \\end{align*}\\] Create a vector cdf in R with \\(cdf[x+1] = P(K_0 \\leq x)\\) for \\(x \\in 0, \\ldots, 105\\). Hint: cumprod in R; cumprod(c(2, 3, 5, 7)) [1] 2 6 30 210 The Figure below shows the CDF of the whole lifetime \\(K_0\\) for a newborn. Since \\(K_0\\) is a discrete random variable this CDF is a step function. The following Theorem presents a strategy for simulating from a discrete distribution given its CDF. Given a CDF \\(F_X\\) of a discrete distribution with possible outcomes \\(x_1, \\ldots, x_n\\) and a uniformly distributed random variable \\(U\\) on \\([0, 1]\\). Then \\(F_X\\) is the CDF of the random variable \\(X = \\max_{x_i} \\{ F_X(x_i) \\leq U \\}\\). Complete the code below to simulate a single observation from \\(K_0\\). This function performs the following steps: Simulate an observation \\(u\\) from a random variable \\(U \\in [0, 1]\\); Loop over all posible ages \\(x\\) in increasing order; Find the smallest age \\(x\\) for which \\(F_{K_0}(x) &gt; u\\); The previous age \\(x-1\\) is the largest age for which \\(F_{K_0}(x) \\leq u\\). By the theorem above, \\(x-1\\) is a simulation for the whole age. simulWholeAge &lt;- function() { # simulate one observation u from a uniform distribution on [0, 1] u &lt;- runif(1); # loop over all posible values F_X(x_i) in increasing order for(j in 1:length(cdf)) { # Check F_X(x_i) &gt; U if( ??? ) { # If F_X(x_i) &gt; u, then the previous value x_{i-1} is the largest value for which F_X(x_{i-1}) &lt;= u return( ??? ); } } } Use simulWholeAge to generate 10000 observations from the distribution of \\(K_0\\); # the number of simulations nsim &lt;- 10000 # initialise a vector with nsim zeroes. K &lt;- rep(0, nsim) for(i in 1:nsim) { # Simulate the age of death for a single individual; K[i] &lt;- ??? } Use ggplot to create a histogram of your simulation for the whole life time. See geom_histogram. cdf &lt;- 1 - cumprod(1-q) nsim &lt;- 10000 K &lt;- rep(0, nsim) simulWholeAge &lt;- function() { u &lt;- runif(1); for(j in 1:length(cdf)) { if(u &lt; cdf[j]) { return(j-1); } } } for(i in 1:nsim) { K[i] &lt;- simulWholeAge(); } ggplot() + geom_histogram(aes(K), col = &quot;#99CCFF&quot;, fill = &quot;#99CCFF&quot;) + theme_bw() + ggtitle(&quot;Histogram lifetime male, Belgium 2017&quot;) + xlab(&#39;age&#39;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 4.3.3 Simulate the future lifetime You will now simulate the exact (instead of integer) remaining lifetime \\(T_0\\) of a newborn. Note that \\(T_0\\) can be written as (explain!) \\(T_0 = \\lfloor T_0 \\rfloor + (T_0-\\lfloor T_0 \\rfloor)\\) where \\(\\lfloor T_0 \\rfloor \\in \\mathbb{N}\\) and \\((T_0-\\lfloor T_0 \\rfloor) \\in [0,1)\\). You will need to make an assumption regarding the probability of dying at non-integer ages. You assume a constant force of mortality between integer ages, i.e. \\[ P(T_0 \\leq t + x \\mid T_0 &gt; x) = 1 - \\exp(-\\mu_x \\cdot t), \\quad \\text{for} \\quad x \\in \\mathbb{N} \\quad \\text{and} \\quad t \\in [0, 1)\\] for some intensity \\(\\mu_x\\) of dying at age \\(x\\). More insight behind this assumption will be given in the courses on Life Insurance Mathematics and Advanced Life Insurance Mathematics. The intensity \\(\\mu_x\\) can be found by taking \\(t = 1\\) in the above expression, \\[ 1 - \\exp(-\\mu_x) = P(T_0 \\leq 1 + x \\mid T_0 &gt; x) = q_x, \\] from which \\[ \\mu_x = -\\log(1-q_x).\\] From this you compute the distribution of \\(T_0 - K_0 \\mid K_0 = x\\), \\[\\begin{align} P(T_0 - K_0 \\leq t \\mid K_0 = x) &amp;= P(T_0 \\leq x + t \\mid T_0 \\geq x,\\ T _0\\leq x+1) \\\\ &amp;= \\frac{P(T_0 \\leq x+t \\mid T_0 \\geq x)}{P(T_0 \\leq x+1 \\mid T_0 \\geq x)} \\\\ &amp;= \\frac{1- (1-q_x)^t}{q_x}. \\end{align}\\] In the previous section, you simulated an observation for the whole lifetime \\(K_0\\) of a newborn. You will now simulate a corresponding observation from the distribution of \\(T_0-K_0 \\mid K_0\\). The distribution for \\(T_0 - K_0 \\mid K_0\\) is continuous and the classical Probability Integral Transform can be used. Show that the inverted CDF of \\(T_0 - K_0 \\mid K_0\\) is given by \\[ F^{-1}_{T_0-K_0\\mid K_0}(y \\mid x) = \\frac{\\log(1-q_x \\cdot y)}{\\log(1-q_x)}; \\] Simulate for each observation of \\(K_0\\) a sample from the distribution \\(T_0-K_0 \\mid K_0\\). Verify that the correct value of \\(q_x\\) is used for each sample; Add the simulations from \\(K_0\\) and \\(T_0-K_0 \\mid K_0\\) to obtain simulations for the remaining lifetime \\(T_0\\) of a newborn. # @2 u &lt;- runif(nsim) remainder &lt;- log(1 - q[K+1] * u) / log(1-q[K+1]) sim &lt;- K + remainder 4.3.4 Visualize the data Visualize the density of the remaining life time \\(T_0\\) of a newborn with ggplot. ggplot() + geom_density(aes(sim), col = &quot;#99CCFF&quot;, fill = &quot;#99CCFF&quot;) + theme_bw() + ggtitle(&quot;Density lifetime male, Belgium 2017&quot;) "],
["glms.html", "5 Generalized Linear Models 5.1 Modelling count data with Poisson regression models 5.2 Overdispersed Poisson regression 5.3 Negative Binomial regression", " 5 Generalized Linear Models You’ll now study the use of Generalized Linear Models in R for insurance ratemaking. You focus first on the example from Rob Kaas’ et al. (2008) Modern Actuarial Risk Theory book (see Section 9.5 in this book), with simulated claim frequency data. 5.1 Modelling count data with Poisson regression models 5.1.1 A first data set This example uses artifical, simulated data. You consider data on claim frequencies, registered on 54 risk cells over a period of 7 years. n gives the number of claims, and expo the corresponding number of policies in a risk cell; each policy is followed over a period of 7 years and n is the number of claims reported over this total period. n &lt;- scan(n = 54) 1 8 10 8 5 11 14 12 11 10 5 12 13 12 15 13 12 24 12 11 6 8 16 19 28 11 14 4 12 8 18 3 17 6 11 18 12 3 10 18 10 13 12 31 16 16 13 14 8 19 20 9 23 27 expo &lt;- scan(n = 54) * 7 10 22 30 11 15 20 25 25 23 28 19 22 19 21 19 16 18 29 25 18 20 13 26 21 27 14 16 11 23 26 29 13 26 13 17 27 20 18 20 29 27 24 23 26 18 25 17 29 11 24 16 11 22 29 n expo [1] 1 8 10 8 5 11 14 12 11 10 5 12 13 12 15 13 12 24 12 11 6 8 16 19 28 [26] 11 14 4 12 8 18 3 17 6 11 18 12 3 10 18 10 13 12 31 16 16 13 14 8 19 [51] 20 9 23 27 [1] 70 154 210 77 105 140 175 175 161 196 133 154 133 147 133 112 126 203 175 [20] 126 140 91 182 147 189 98 112 77 161 182 203 91 182 91 119 189 140 126 [39] 140 203 189 168 161 182 126 175 119 203 77 168 112 77 154 203 The goal is to illustrate ratemaking by explaining the expected number of claims as a function of a set of observable risk factors. Since artificial data are used in this example, you use simulated or self constructed risk factors. 4 factor variables are created, the sex of the policyholder (1=female and 2=male), the region where she lives (1=countryside, 2=elsewhere and 3=big city), the type of car (1=small, 2=middle and 3=big) and job class of the insured (1=civil servant/actuary/…, 2=in-between and 3=dynamic drivers). You use the R instruction rep() to construct these risk factors. In total 54 risk cells are created in this way. Note that you use the R instruction as.factor() to specify the risk factors as factor (or: categorical) covariates. sex &lt;- as.factor(rep(1:2, each=27, len=54)) region &lt;- as.factor(rep(1:3, each=9, len=54)) type &lt;- as.factor(rep(1:3, each=3, len=54)) job &lt;- as.factor(rep(1:3, each=1, len=54)) sex [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 [39] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 Levels: 1 2 region [1] 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 2 2 [39] 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 Levels: 1 2 3 type [1] 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 1 1 [39] 1 2 2 2 3 3 3 1 1 1 2 2 2 3 3 3 Levels: 1 2 3 job [1] 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 [39] 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 Levels: 1 2 3 5.1.2 Fit a Poisson GLM The response variable \\(N_i\\) is the number of claims reported on risk cell i, hence it is reasonable to assume a Poisson distribution for this random variable. You fit the following Poisson GLM to the data \\[\\begin{eqnarray*} N_i &amp;\\sim&amp; \\text{POI}(d_i \\cdot \\lambda_i) \\end{eqnarray*}\\] where \\(\\lambda_i = \\exp{(\\boldsymbol{x}^{&#39;}_i\\boldsymbol{\\beta})}\\) and \\(d_i\\) is the exposure for risk cell \\(i\\). In R you use the instruction glm to fit a GLM. Covariates are listed with +, and the log of expo is used as an offset. Indeed, \\[\\begin{eqnarray*} N_i &amp;\\sim&amp; \\text{POI}(d_i \\cdot \\lambda_i) \\\\ &amp;= &amp; \\text{POI}(\\exp{(\\boldsymbol{x}^{&#39;}_i \\boldsymbol{\\beta}+\\log{(d_i)})}) \\end{eqnarray*}\\] The R instruction to fit this GLM (with sex, region, type and job the factor variables that construct the linear predictor) then goes as follows g1 &lt;- glm(n ~ sex + region + type + job + offset(log(expo)), fam = poisson(link = log)) where the argument fam= indicates the distribution from the exponential family that is assumed. In this case you work with the Poisson distribution with logarithmic link (which is the default link in R). All available distributions and their default link functions are listed here http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html. You store the results of the glm fit in the object g1. You consult this object with the summary instruction summary(g1) Call: glm(formula = n ~ sex + region + type + job + offset(log(expo)), family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -1.9278 -0.6303 -0.0215 0.5380 2.3000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0996 0.1229 -25.21 &lt; 2e-16 *** sex2 0.1030 0.0763 1.35 0.1771 region2 0.2347 0.0992 2.36 0.0180 * region3 0.4643 0.0965 4.81 1.5e-06 *** type2 0.3946 0.1017 3.88 0.0001 *** type3 0.5844 0.0971 6.02 1.8e-09 *** job2 -0.0362 0.0970 -0.37 0.7091 job3 0.0607 0.0926 0.66 0.5121 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 41.93 on 46 degrees of freedom AIC: 288.2 Number of Fisher Scoring iterations: 4 This summary of a glm fit lists (among others) the following items: the covariates used in the model, the corresponding estimates for the regression parameters (\\(\\boldsymbol{\\hat{\\beta}}\\)), their standard errors, \\(z\\) statistics and corresponding \\(P\\) values; the dispersion parameter used; for the standard Poisson regression model this dispersion parameter is equal to 1, as indicated in the R output; the null deviance - the deviance of the model that uses only an intercept - and the residual deviance - the deviance of the current model; the null deviance corresponds to \\(53\\) degrees of freedom, that is \\(54-1\\) where \\(54\\) is the number of observations used and \\(1\\) the number of parameters (here: just the intercept); the residual deviance corresponds to \\(54-8=46\\) degrees of freedom, since it uses \\(8\\) parameters; the AIC calculated for the considered regression model; the number of Fisher’s iterations needed to get convergence of the iterative numerical method to calculate the MLEs of the regression parameters in \\(\\boldsymbol{\\beta}\\). The instruction names shows the names of the variables stored within a glm object. One of these variables is called coef and contains the vector of regression parameter estimates (\\(\\hat{\\boldsymbol{\\beta}}\\)). It can be extracted with the instruction g1$coef. names(g1) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;fitted.values&quot; [4] &quot;effects&quot; &quot;R&quot; &quot;rank&quot; [7] &quot;qr&quot; &quot;family&quot; &quot;linear.predictors&quot; [10] &quot;deviance&quot; &quot;aic&quot; &quot;null.deviance&quot; [13] &quot;iter&quot; &quot;weights&quot; &quot;prior.weights&quot; [16] &quot;df.residual&quot; &quot;df.null&quot; &quot;y&quot; [19] &quot;converged&quot; &quot;boundary&quot; &quot;model&quot; [22] &quot;call&quot; &quot;formula&quot; &quot;terms&quot; [25] &quot;data&quot; &quot;offset&quot; &quot;control&quot; [28] &quot;method&quot; &quot;contrasts&quot; &quot;xlevels&quot; g1$coef (Intercept) sex2 region2 region3 type2 type3 -3.09959 0.10303 0.23468 0.46434 0.39463 0.58443 job2 job3 -0.03617 0.06072 Other variables can be consulted in a similar way. For example, fitted values at the original level are \\(\\hat{\\mu}_i=\\exp{(\\hat{\\eta}_i)}\\) where the fitted values at the level of the linear predictor are stored in \\(\\hat{\\eta}_i=\\log{(d_i)}+\\boldsymbol{x}^{&#39;}_i\\hat{\\boldsymbol{\\beta}}\\). You then plot the fitted values versus the observed number of claims n. You add two reference lines: the diagonal and the least squares line. g1$fitted.values 1 2 3 4 5 6 7 8 9 10 11 3.155 6.694 10.057 5.149 6.772 9.948 14.149 13.646 13.832 11.170 7.310 12 13 14 15 16 17 18 19 20 21 22 9.326 11.247 11.989 11.951 11.450 12.424 22.053 12.548 8.713 10.667 9.682 23 24 25 26 27 28 29 30 31 32 33 18.676 16.619 24.311 12.158 15.308 3.847 7.758 9.662 15.049 6.506 14.336 34 35 36 37 38 39 40 41 42 43 44 8.156 10.286 17.999 8.844 7.677 9.398 19.029 17.087 16.734 18.246 19.893 45 46 47 48 49 50 51 52 53 54 15.173 13.909 9.122 17.145 9.081 19.110 14.036 10.979 21.179 30.758 g1$linear.predictors 1 2 3 4 5 6 7 8 9 10 11 12 13 1.149 1.901 2.308 1.639 1.913 2.297 2.650 2.613 2.627 2.413 1.989 2.233 2.420 14 15 16 17 18 19 20 21 22 23 24 25 26 2.484 2.481 2.438 2.520 3.093 2.530 2.165 2.367 2.270 2.927 2.811 3.191 2.498 27 28 29 30 31 32 33 34 35 36 37 38 39 2.728 1.347 2.049 2.268 2.711 1.873 2.663 2.099 2.331 2.890 2.180 2.038 2.240 40 41 42 43 44 45 46 47 48 49 50 51 52 2.946 2.838 2.817 2.904 2.990 2.720 2.633 2.211 2.842 2.206 2.950 2.642 2.396 53 54 3.053 3.426 plot(g1$fitted.values, n, xlab = &quot;Fitted values&quot;, ylab = &quot;Observed claims&quot;) abline(lm(g1$fitted ~ n), col=&quot;light blue&quot;, lwd=2) abline(0, 1, col = &quot;dark blue&quot;, lwd=2) To extract the AIC you use AIC(g1) [1] 288.2 5.1.3 The use of exposure The use of expo, the exposure measure, in a Poisson GLM often leads to confusion. For example, the following glm instruction uses a transformed response variable \\(n/expo\\) g2 &lt;- glm(n/expo ~ sex+region+type+job,fam=poisson(link=log)) summary(g2) Call: glm(formula = n/expo ~ sex + region + type + job, family = poisson(link = log)) Deviance Residuals: Min 1Q Median 3Q Max -0.16983 -0.05628 -0.00118 0.04680 0.17676 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.1392 1.4846 -2.11 0.034 * sex2 0.1007 0.9224 0.11 0.913 region2 0.2624 1.2143 0.22 0.829 region3 0.4874 1.1598 0.42 0.674 type2 0.4095 1.2298 0.33 0.739 type3 0.5757 1.1917 0.48 0.629 job2 -0.0308 1.1502 -0.03 0.979 job3 0.0957 1.1150 0.09 0.932 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 0.77537 on 53 degrees of freedom Residual deviance: 0.31739 on 46 degrees of freedom AIC: Inf Number of Fisher Scoring iterations: 5 and the object g3 stores the result of a Poisson fit on the same response variable, while taking expo into account as weights in the likelihood. g3 &lt;- glm(n/expo ~ sex+region+type+job,weights=expo,fam=poisson(link=log)) summary(g3) Call: glm(formula = n/expo ~ sex + region + type + job, family = poisson(link = log), weights = expo) Deviance Residuals: Min 1Q Median 3Q Max -1.9278 -0.6303 -0.0215 0.5380 2.3000 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0996 0.1229 -25.21 &lt; 2e-16 *** sex2 0.1030 0.0763 1.35 0.1771 region2 0.2347 0.0992 2.36 0.0180 * region3 0.4643 0.0965 4.81 1.5e-06 *** type2 0.3946 0.1017 3.88 0.0001 *** type3 0.5844 0.0971 6.02 1.8e-09 *** job2 -0.0362 0.0970 -0.37 0.7091 job3 0.0607 0.0926 0.66 0.5121 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 41.93 on 46 degrees of freedom AIC: Inf Number of Fisher Scoring iterations: 5 Based on this output you conclude that g1 (with the log of exposure as offset in the linear predictor) and g3 are the same, but g2 is not. The mathematical explanation for this observation is given in the note ‘WeightsInGLMs.pdf’ available from Katrien’s lecture notes (available upon request). 5.1.4 Analysis of deviance for GLMs 5.1.4.1 The basics You now focus on the selection of variables within a GLM based on a drop in deviance analysis. Your starting point is the GLM object g1 and the anova instruction. g1 &lt;- glm(n ~ 1 + region + type + job, poisson, offset = log(expo)) anova(g1, test=&quot;Chisq&quot;) Analysis of Deviance Table Model: poisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) NULL 53 104.7 region 2 21.6 51 83.1 2.0e-05 *** type 2 38.2 49 44.9 5.1e-09 *** job 2 1.2 47 43.8 0.55 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The analysis of deviance table first summarizes the Poisson GLM object (response n, link is log, family is poisson). The table starts with the deviance of the NULL model (just using an intercept), and then adds risk factors sequentially. Recall that in this example only factor covariates are present. Adding region (which has three levels, and requires two dummy variables) to the NULL model causes a drop in deviance of 21.597, corresponding to 54-1-2 degrees of freedom and a resulting (residual) deviance of 83.135. The drop in deviance test allows to test whether the model term region is significant. That is: \\[ H_0: \\beta_{\\text{region}_2}=0\\ \\text{and}\\ \\beta_{\\text{region}_3}=0. \\] The distribution of the corresponding test statistic is a Chi-squared distribution with 2 (i.e 53-51) degrees of freedom. The corresponding \\(P\\)-value is 2.043e-05. Hence, the model using region and the intercept is preferred above the NULL model. We can verify the \\(P\\)-value by calculating the following probability \\[ Pr(X &gt; 21.597)\\ \\text{with}\\ X \\sim \\chi^2_{2}.\\] Indeed, this is the probability - under \\(H_0\\) - to obtain a value of the test statistic that is the same or more extreme than the actual observed value of the test statistic. Calculations in R are as follows: # p-value for region 1 - pchisq(21.597, 2) [1] 2.043e-05 # or pchisq(21.597, 2, lower.tail = FALSE) [1] 2.043e-05 Continuing the discussion of the above printed anova table, the next step is to add type to the model using an intercept and region. This causes a drop in deviance of 38.195. You conclude that also type is a significant model term. The last step adds job to the existing model (with intercept, region and type). You conclude that job does not have a significant impact when explaining the expected number of claims. Based on this analysis of deviance table region and type seem to be relevant risk factors, but job is not, when explaining the expected number of claims. The Chi-squared distribution is used here, since the regular Poisson regression model does not require the estimation of a dispersion parameter. anova(g1,test=&quot;Chisq&quot;) The setting changes when the dispersion parameter is unknown and should be estimated. If you run the analysis of deviance for glm object g1 with the F distribution as distribution for the test statistic, you obtain: # what if we use &#39;F&#39; instead of &#39;Chisq&#39;? anova(g1,test=&quot;F&quot;) Warning in anova.glm(g1, test = &quot;F&quot;): using F test with a &#39;poisson&#39; family is inappropriate Analysis of Deviance Table Model: poisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) NULL 53 104.7 region 2 21.6 51 83.1 10.80 2.0e-05 *** type 2 38.2 49 44.9 19.10 5.1e-09 *** job 2 1.2 47 43.8 0.59 0.55 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # not appropriate for regular Poisson regression, see Warning message in the console! and a Warning message is printed in the console that says # Warning message: # In anova.glm(g1, test = &quot;F&quot;) : # using F test with a &#39;poisson&#39; family is inappropriate It is insightful to understand how the output shown for the \\(F\\) statistic and corresponding \\(P\\)-value is calculated. For example, the drop in deviance test comparing the NULL model viz a model using an intercept and region corresponds to an observed test statistic of 10.7985. The calculation of the \\(F\\) statistic requires \\[ \\frac{\\text{Drop-in-deviance}/q}{\\hat{\\phi}}, \\] where \\(q\\) is the difference in degrees of freedom between the compared models and \\(\\hat{\\phi}\\) is the estimate for the dispersion parameter. In this example \\(F\\) corresponding to region is calculated as (21.597/2)/1 [1] 10.8 However, as explained, since the model investigated has a known dispersion, the Chi-squared test is most appropriate here. More details are here: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/anova.glm.html. 5.1.5 An example You are now ready to study a complete analysis-of-deviance table. This table investigates 10 possible model specifications g1-g10. # construct an analysis-of-deviance table g1 &lt;- glm(n ~ 1, poisson , offset=log(expo)) g2 &lt;- glm(n ~ sex, poisson , offset=log(expo)) g3 &lt;- glm(n ~ sex+region, poisson, offset=log(expo)) g4 &lt;- glm(n ~ sex+region+sex:region, poisson, offset=log(expo)) g5 &lt;- glm(n ~ type, poisson, offset=log(expo)) g6 &lt;- glm(n ~ region, poisson, offset=log(expo)) g7 &lt;- glm(n ~ region+type, poisson, offset=log(expo)) g8 &lt;- glm(n ~ region+type+region:type, poisson, offset=log(expo)) g9 &lt;- glm(n ~ region+type+job, poisson, offset=log(expo)) g10 &lt;- glm(n ~ region+type+sex, poisson, offset=log(expo)) For example, the residual deviance obtained with model g8 (using intercept, region, type and the interaction of region and type) is 42.4, see summary(g8) Call: glm(formula = n ~ region + type + region:type, family = poisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.8296 -0.4893 -0.0622 0.5377 1.8974 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.9887 0.1525 -19.60 &lt;2e-16 *** region2 0.1499 0.2061 0.73 0.467 region3 0.4216 0.1927 2.19 0.029 * type2 0.4338 0.1985 2.19 0.029 * type3 0.4520 0.1927 2.35 0.019 * region2:type2 -0.0808 0.2664 -0.30 0.762 region3:type2 -0.0223 0.2537 -0.09 0.930 region2:type3 0.2556 0.2562 1.00 0.318 region3:type3 0.1086 0.2449 0.44 0.657 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.732 on 53 degrees of freedom Residual deviance: 42.412 on 45 degrees of freedom AIC: 290.7 Number of Fisher Scoring iterations: 4 g8$deviance [1] 42.41 Using the technique of drop in deviance analysis you compare the models that are nested (!!) and decide which model specification is the preferred one. To do this, one can run multiple anova instructions such as anova(g1, g2, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: n ~ 1 Model 2: n ~ sex Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 53 105 2 52 103 1 1.93 0.17 which compares nested models g1 and g2, or g7 and g8 anova(g7, g8, test = &quot;Chisq&quot;) Analysis of Deviance Table Model 1: n ~ region + type Model 2: n ~ region + type + region:type Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 49 44.9 2 45 42.4 4 2.53 0.64 5.2 Overdispersed Poisson regression The overdispersed Poisson model builds a regression model for the mean of the response variable \\[ EN_i = \\exp{(\\log d_i + \\boldsymbol{x}_i^{&#39;}\\boldsymbol{\\beta})} \\] and expressses the variance as \\[ \\text{Var}(N_i) = \\phi \\cdot EN_i, \\] with \\(N_i\\) the number of claims reported by policyholder \\(i\\) and \\(\\phi\\) an unknown dispersion parameter that should be estimated. This is called a quasi-Poisson model (see http://stat.ethz.ch/R-manual/R-patched/library/stats/html/family.html) and Section 1 in http://data.princeton.edu/wws509/notes/c4a.pdf for a more detailed explanation. To illustrate the differences between a regular Poisson and an overdispersed Poisson model, we fit the models g.poi and g.quasi: g.poi &lt;- glm(n ~ 1 + region + type, poisson, offset = log(expo)) summary(g.poi) Call: glm(formula = n ~ 1 + region + type, family = poisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.9233 -0.6564 -0.0573 0.4790 2.3144 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.0313 0.1015 -29.86 &lt; 2e-16 *** region2 0.2314 0.0990 2.34 0.0195 * region3 0.4605 0.0965 4.77 1.8e-06 *** type2 0.3942 0.1015 3.88 0.0001 *** type3 0.5833 0.0971 6.01 1.9e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 44.94 on 49 degrees of freedom AIC: 285.2 Number of Fisher Scoring iterations: 4 g.quasi &lt;- glm(n ~ 1 + region + type, quasipoisson, offset = log(expo)) summary(g.quasi) Call: glm(formula = n ~ 1 + region + type, family = quasipoisson, offset = log(expo)) Deviance Residuals: Min 1Q Median 3Q Max -1.9233 -0.6564 -0.0573 0.4790 2.3144 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.0313 0.0961 -31.54 &lt; 2e-16 *** region2 0.2314 0.0938 2.47 0.01715 * region3 0.4605 0.0913 5.04 6.7e-06 *** type2 0.3942 0.0961 4.10 0.00015 *** type3 0.5833 0.0919 6.35 6.8e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 0.8965) Null deviance: 104.73 on 53 degrees of freedom Residual deviance: 44.94 on 49 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 4 Parameter estimates in both models are the same, but standard errors (and hence \\(P\\)-values) are not! You also see that g.poi reports z-value whereas g.quasi reports t-value, because the latter model estimates an extra parameter, i.e. the dispersion parameter. Various methods are available to estimate the dispersion parameter, e.g. \\[ \\hat{\\phi} = \\frac{\\text{Deviance}}{n-(p+1)}\\] and \\[ \\hat{\\phi} = \\frac{\\text{Pearson}\\ \\chi^2}{n-(p+1)}\\] where \\(p+1\\) is the total number of parameters (including the intercept) used in the considered model. The (residual) deviance is the deviance of the considered model and can also be obtained as the sum of squared deviance residuals. The Pearson \\(\\chi^2\\) statistic is the sum of the squared Pearson residuals. The latter is the default in R. Hence, you can verify the dispersion parameter of 0.896 as printed in the summary of g.quasi: # dispersion parameter in g is estimated as follows phi &lt;- sum(residuals(g.poi, &quot;pearson&quot;)^2)/g.poi$df.residual phi [1] 0.8965 Since \\(\\hat{\\phi}\\) is less than 1, the result seems to indicate underdispersion. However, as discussed in Section 2.4 ‘Overdispersion’ in the book of Denuit et al. (2007), real data on reported claim counts very often reveal overdispersion. The counterintuitive result that is obtained here is probably due to the fact that artificial, self-constructed data are used. When going from g.poi (regular Poisson) to g.quasi the standard errors are changed as follows: \\[ \\text{SE}_{\\text{Q-POI}} = \\sqrt{\\hat{\\phi}} \\cdot \\text{SE}_{\\text{POI}},\\] where \\(\\text{Q-POI}\\) is for quasi-Poisson. As a last step, you run the analysis of deviance for the quasi-Poisson model: anova(g.quasi, test = &quot;F&quot;) Analysis of Deviance Table Model: quasipoisson, link: log Response: n Terms added sequentially (first to last) Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) NULL 53 104.7 region 2 21.6 51 83.1 12.0 5.6e-05 *** type 2 38.2 49 44.9 21.3 2.2e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For example, the \\(F\\)-statistic for region is calculated as F &lt;- (21.597/2)/phi F [1] 12.04 and the corresponding \\(P\\)-value is pf(F, 2, 49, lower.tail = FALSE) [1] 5.564e-05 5.3 Negative Binomial regression You now focus on the use of yet another useful count regression model, that is the Negative Binomial regression model. The routine to fit a NB regression model is available in the package MASS and is called glm.nb, see https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/glm.nb.html # install.packages(&quot;MASS&quot;) library(MASS) g.nb &lt;- glm.nb(n ~ 1+region+sex+offset(log(expo))) summary(g.nb) Call: glm.nb(formula = n ~ 1 + region + sex + offset(log(expo)), init.theta = 26.38897379, link = log) Deviance Residuals: Min 1Q Median 3Q Max -2.6222 -0.6531 -0.0586 0.6587 2.3542 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.7342 0.1019 -26.84 &lt; 2e-16 *** region2 0.2359 0.1195 1.97 0.04837 * region3 0.4533 0.1176 3.85 0.00012 *** sex2 0.1049 0.0939 1.12 0.26392 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(26.39) family taken to be 1) Null deviance: 71.237 on 53 degrees of freedom Residual deviance: 54.917 on 50 degrees of freedom AIC: 316.1 Number of Fisher Scoring iterations: 1 Theta: 26.4 Std. Err.: 15.3 2 x log-likelihood: -306.1 "],
["claims-reserving.html", "6 Claims reserving 6.1 Goals 6.2 Import a run-off triangle 6.3 Mack’s Chain-Ladder calculations 6.4 GLM analysis of a run-off triangle 6.5 Bootstrap analysis 6.6 More info", " 6 Claims reserving 6.1 Goals You’ll focus on reserving analytics with run-off triangles, including the following steps: importing a run-off triangle visualizing the data in the triangle basic chain-ladder calculations: the Mack approach setting up a bootstrap analysis to simulate from the predictive distribution of the outstanding loss amount while incorporating parameter and process uncertainty. 6.2 Import a run-off triangle 6.2.1 Using scan You can load a run-off triangle in many different ways. Here is a first strategy, using the scan() function in R. Xij &lt;- scan(n=55) 357848 766940 610542 482940 527326 574398 146342 139950 227229 67948 352118 884021 933894 1183289 445745 320996 527804 266172 425046 290507 1001799 926219 1016654 750816 146923 495992 280405 310608 1108250 776189 1562400 272482 352053 206286 443160 693190 991983 769488 504851 470639 396132 937085 847498 805037 705960 440832 847631 1131398 1063269 359480 1061648 1443370 376686 986608 344014 Exercise: verify the structure of object Xij. What happened? You will now reshape the flat vector Xij into a data frame that can be used for claims reserving with GLMs. n &lt;- length(Xij) TT &lt;- trunc(sqrt(2*n)) # row.nrs i &lt;- rep(1:TT, TT:1) # col.nrs j &lt;- sequence(TT:1) # calendar.nrs c &lt;- i+j-1 # as.factor i &lt;- as.factor(i) j &lt;- as.factor(j) c &lt;- as.factor(c) # combine into a data frame my_triangle_data &lt;- data.frame(Xij, i, j, c) Exercise: verify the structure of the resulting data frame inspect the first 10 rows using head. You now prepare some first visual inspections of the data stored in my_triangle_data. par(mfrow = c(2, 2)) boxplot(Xij ~ i, col = &quot;grey&quot;, main = &quot;Boxplot of payments vs AY&quot;) boxplot(Xij ~ j, col = &quot;grey&quot;, main = &quot;Boxplot of payments vs DY&quot;) boxplot(Xij ~ c, col = &quot;grey&quot;, main = &quot;Boxplot of payments vs CY&quot;) par(mfrow = c(1, 1)) 6.2.2 Using the chainladder package The ChainLadder package is a well-developed and highly useful package for claims reserving in R. Among others the package brings functions covering: Mack chain-ladder, Munich chain-ladder and bootstrap models General multivariate chain ladder-models Loss development factor fitting and Cape Cod models Generalized linear models One year claims development result functions Utility functions to: convert tables into triangles and triangles into tables convert cumulative into incremental and incremental into cumulative triangles visualise triangles. You’ll now load the triangle introduced above with the ChainLadder package. Here is how this works. path &lt;- file.path(&quot;C:/Users/u0043788/Dropbox/Risk modelling course Ljubljana/Bookdown/data&quot;) path.triangle &lt;- file.path(path, &quot;TaylorAshe.txt&quot;) my_triangle_data_set &lt;- read.table(path.triangle, header = TRUE, sep=&quot;\\t&quot;) Using the as.triangle function from the ChainLadder package you can reshape the data frame into a ‘triangle’. #install.packages(&quot;ChainLadder&quot;) library(ChainLadder) # prepare triangle my_triangle &lt;- as.triangle(my_triangle_data_set, origin = &quot;NumAY&quot;, dev = &quot;NumDY&quot;, value = &quot;Paym&quot;) my_triangle NumDY NumAY 1 2 3 4 5 6 7 8 9 10 1 357848 766940 610542 482940 527326 574398 146342 139950 227229 67948 2 352118 884021 933894 1183289 445745 320996 527804 266172 425046 NA 3 290507 1001799 926219 1016654 750816 146923 495992 280405 NA NA 4 310608 1108250 776189 1562400 272482 352053 206286 NA NA NA 5 443160 693190 991983 769488 504851 470639 NA NA NA NA 6 396132 937085 847498 805037 705960 NA NA NA NA NA 7 440832 847631 1131398 1063269 NA NA NA NA NA NA 8 359480 1061648 1443370 NA NA NA NA NA NA NA 9 376686 986608 NA NA NA NA NA NA NA NA 10 344014 NA NA NA NA NA NA NA NA NA Exercise: interpret the arguments of the function as.triangle what is the structure of the R object my_triangle? Here is an illustration of some useful functions from the package. Can you figure out what they do? my_triangle_cum &lt;- incr2cum(my_triangle) my_triangle_cum NumDY NumAY 1 2 3 4 5 6 7 8 9 1 357848 1124788 1735330 2218270 2745596 3319994 3466336 3606286 3833515 2 352118 1236139 2170033 3353322 3799067 4120063 4647867 4914039 5339085 3 290507 1292306 2218525 3235179 3985995 4132918 4628910 4909315 NA 4 310608 1418858 2195047 3757447 4029929 4381982 4588268 NA NA 5 443160 1136350 2128333 2897821 3402672 3873311 NA NA NA 6 396132 1333217 2180715 2985752 3691712 NA NA NA NA 7 440832 1288463 2419861 3483130 NA NA NA NA NA 8 359480 1421128 2864498 NA NA NA NA NA NA 9 376686 1363294 NA NA NA NA NA NA NA 10 344014 NA NA NA NA NA NA NA NA NumDY NumAY 10 1 3901463 2 NA 3 NA 4 NA 5 NA 6 NA 7 NA 8 NA 9 NA 10 NA plot(my_triangle) plot(my_triangle_cum) plot(my_triangle, lattice = TRUE) plot(my_triangle_cum, lattice = TRUE) getLatestCumulative(my_triangle_cum) 1 2 3 4 5 6 7 8 9 10 3901463 5339085 4909315 4588268 3873311 3691712 3483130 2864498 1363294 344014 attr(,&quot;latestcol&quot;) 1 2 3 4 5 6 7 8 9 10 10 9 8 7 6 5 4 3 2 1 attr(,&quot;rowsname&quot;) [1] &quot;NumAY&quot; attr(,&quot;colnames&quot;) [1] &quot;10&quot; &quot;9&quot; &quot;8&quot; &quot;7&quot; &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; attr(,&quot;colsname&quot;) [1] &quot;NumDY&quot; 6.3 Mack’s Chain-Ladder calculations The ChainLadder package can easily perform Mack’s calculations (see the lecture notes by Katrien for more details). M &lt;- MackChainLadder(my_triangle_cum, est.sigma = &quot;Mack&quot;) M MackChainLadder(Triangle = my_triangle_cum, est.sigma = &quot;Mack&quot;) Latest Dev.To.Date Ultimate IBNR Mack.S.E CV(IBNR) 1 3,901,463 1.0000 3,901,463 0 0 NaN 2 5,339,085 0.9826 5,433,719 94,634 75,535 0.798 3 4,909,315 0.9127 5,378,826 469,511 121,699 0.259 4 4,588,268 0.8661 5,297,906 709,638 133,549 0.188 5 3,873,311 0.7973 4,858,200 984,889 261,406 0.265 6 3,691,712 0.7223 5,111,171 1,419,459 411,010 0.290 7 3,483,130 0.6153 5,660,771 2,177,641 558,317 0.256 8 2,864,498 0.4222 6,784,799 3,920,301 875,328 0.223 9 1,363,294 0.2416 5,642,266 4,278,972 971,258 0.227 10 344,014 0.0692 4,969,825 4,625,811 1,363,155 0.295 Totals Latest: 34,358,090.00 Dev: 0.65 Ultimate: 53,038,945.61 IBNR: 18,680,855.61 Mack.S.E 2,447,094.86 CV(IBNR): 0.13 # get development factors M$f [1] 3.491 1.747 1.457 1.174 1.104 1.086 1.054 1.077 1.018 1.000 # get the estimates for \\sigma^2 M$sigma^2 [1] 160280.3 37736.9 41965.2 15182.9 13731.3 8185.8 446.6 1147.4 [9] 446.6 # get the full triangle M$FullTriangle dev origin 1 2 3 4 5 6 7 8 9 1 357848 1124788 1735330 2218270 2745596 3319994 3466336 3606286 3833515 2 352118 1236139 2170033 3353322 3799067 4120063 4647867 4914039 5339085 3 290507 1292306 2218525 3235179 3985995 4132918 4628910 4909315 5285148 4 310608 1418858 2195047 3757447 4029929 4381982 4588268 4835458 5205637 5 443160 1136350 2128333 2897821 3402672 3873311 4207459 4434133 4773589 6 396132 1333217 2180715 2985752 3691712 4074999 4426546 4665023 5022155 7 440832 1288463 2419861 3483130 4088678 4513179 4902528 5166649 5562182 8 359480 1421128 2864498 4174756 4900545 5409337 5875997 6192562 6666635 9 376686 1363294 2382128 3471744 4075313 4498426 4886502 5149760 5544000 10 344014 1200818 2098228 3057984 3589620 3962307 4304132 4536015 4883270 dev origin 10 1 3901463 2 5433719 3 5378826 4 5297906 5 4858200 6 5111171 7 5660771 8 6784799 9 5642266 10 4969825 # get Mack SE M$Mack.S.E dev origin 1 2 3 4 5 6 7 8 9 10 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 75535 3 0 0 0 0 0 0 0 0 94225 121699 4 0 0 0 0 0 0 0 52792 109210 133549 5 0 0 0 0 0 0 198502 215088 247694 261406 6 0 0 0 0 0 247204 337617 359530 397610 411010 7 0 0 0 0 250737 381475 468091 496372 543209 558317 8 0 0 0 378275 524309 648532 745376 787969 855493 875328 9 0 0 241429 489488 626722 739750 832421 878987 951274 971258 10 0 246656 486186 776148 940318 1066053 1175373 1239733 1337626 1363155 # split parameter and process risk M$Mack.ProcessRisk dev origin 1 2 3 4 5 6 7 8 9 10 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 48832 3 0 0 0 0 0 0 0 0 75052 90524 4 0 0 0 0 0 0 0 45268 89011 102622 5 0 0 0 0 0 0 178062 192597 219267 227880 6 0 0 0 0 0 225149 305242 324745 357179 366582 7 0 0 0 0 229965 347244 423348 448603 489045 500202 8 0 0 0 346712 478565 588507 673019 711125 770192 785741 9 0 0 226818 457429 583978 686644 770169 813005 878614 895570 10 0 234816 462237 736128 890565 1007785 1109441 1170034 1261670 1284882 M$Mack.ParameterRisk dev origin 1 2 3 4 5 6 7 8 9 10 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 57628 3 0 0 0 0 0 0 0 0 56970 81338 4 0 0 0 0 0 0 0 27163 63275 85464 5 0 0 0 0 0 0 87733 95756 115215 128078 6 0 0 0 0 0 102068 144266 154280 174690 185867 7 0 0 0 0 99925 157940 199712 212463 236456 248023 8 0 0 0 151271 214185 272494 320359 339405 372388 385759 9 0 0 82715 174234 227486 275226 315855 334127 364636 375893 10 0 75503 150710 246013 301814 347618 388126 409829 444333 455270 sqrt(M$Mack.ProcessRisk^2+M$Mack.ParameterRisk^2) dev origin 1 2 3 4 5 6 7 8 9 10 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 75535 3 0 0 0 0 0 0 0 0 94225 121699 4 0 0 0 0 0 0 0 52792 109210 133549 5 0 0 0 0 0 0 198502 215088 247694 261406 6 0 0 0 0 0 247204 337617 359530 397610 411010 7 0 0 0 0 250737 381475 468091 496372 543209 558317 8 0 0 0 378275 524309 648532 745376 787969 855493 875328 9 0 0 241429 489488 626722 739750 832421 878987 951274 971258 10 0 246656 486186 776148 940318 1066053 1175373 1239733 1337626 1363155 Again, the package can nicely visualize the results. plot(M) plot(M,lattice=TRUE) Exercise: interpret the plots. What do you see? 6.4 GLM analysis of a run-off triangle Next to Mack’s chain ladder calculation, you can also work with the data frame my_triangle_data and build a (overdispersed) Poisson GLM for these data. str(my_triangle_data) &#39;data.frame&#39;: 55 obs. of 4 variables: $ Xij: num 357848 766940 610542 482940 527326 ... $ i : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ j : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ c : Factor w/ 10 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... # Poisson GLM of Xij versus i+j poi_glm &lt;- glm(Xij ~ i + j, poisson(link=log), data = my_triangle_data) summary(poi_glm) Call: glm(formula = Xij ~ i + j, family = poisson(link = log), data = my_triangle_data) Deviance Residuals: Min 1Q Median 3Q Max -464.9 -123.7 -21.7 116.2 494.3 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 12.506405 0.000754 16587.37 &lt; 2e-16 *** i2 0.331272 0.000669 494.85 &lt; 2e-16 *** i3 0.321119 0.000688 466.96 &lt; 2e-16 *** i4 0.305960 0.000701 436.57 &lt; 2e-16 *** i5 0.219316 0.000732 299.46 &lt; 2e-16 *** i6 0.270077 0.000745 362.76 &lt; 2e-16 *** i7 0.372208 0.000761 489.34 &lt; 2e-16 *** i8 0.553333 0.000813 680.38 &lt; 2e-16 *** i9 0.368934 0.001043 353.77 &lt; 2e-16 *** i10 0.242033 0.001864 129.83 &lt; 2e-16 *** j2 0.912526 0.000649 1406.04 &lt; 2e-16 *** j3 0.958831 0.000665 1441.37 &lt; 2e-16 *** j4 1.025997 0.000684 1499.93 &lt; 2e-16 *** j5 0.435276 0.000802 542.81 &lt; 2e-16 *** j6 0.080057 0.000936 85.49 &lt; 2e-16 *** j7 -0.006381 0.001039 -6.14 8.1e-10 *** j8 -0.394452 0.001353 -291.56 &lt; 2e-16 *** j9 0.009378 0.001396 6.72 1.9e-11 *** j10 -1.379907 0.003910 -352.95 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 10699464 on 54 degrees of freedom Residual deviance: 1903014 on 36 degrees of freedom AIC: 1903877 Number of Fisher Scoring iterations: 4 # get coefficients of this regression model coef(poi_glm) (Intercept) i2 i3 i4 i5 i6 12.506405 0.331272 0.321119 0.305960 0.219316 0.270077 i7 i8 i9 i10 j2 j3 0.372208 0.553333 0.368934 0.242033 0.912526 0.958831 j4 j5 j6 j7 j8 j9 1.025997 0.435276 0.080057 -0.006381 -0.394452 0.009378 j10 -1.379907 Exercise: write down the full model specification of this GLM. Using the coefficients stored in poi_glm you want to extract fitted values for upper and lower triangle. # apply inverse link function coefs &lt;- exp(as.numeric(coef(poi_glm))) alpha &lt;- c(1, coefs[2:TT])*coefs[1] alpha [1] 270061 376125 372325 366724 336287 353798 391842 469648 390561 344014 length(alpha) [1] 10 beta &lt;- c(1, coefs[(TT+1):(2*TT-1)]) beta [1] 1.0000 2.4906 2.6086 2.7899 1.5454 1.0833 0.9936 0.6740 1.0094 0.2516 length(beta) [1] 10 # fitted values, original scale orig_fits &lt;- alpha %*% t(beta) dim(orig_fits) [1] 10 10 str(orig_fits) num [1:10, 1:10] 270061 376125 372325 366724 336287 ... orig_fits [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 270061 672617 704494 753438 417350 292571 268344 182035 272606 67948 [2,] 376125 936779 981176 1049342 581260 407474 373732 253527 379669 94634 [3,] 372325 927316 971264 1038741 575388 403358 369957 250966 375833 93678 [4,] 366724 913365 956652 1023114 566731 397290 364391 247190 370179 92268 [5,] 336287 837559 877254 938200 519695 364316 334148 226674 339456 84611 [6,] 353798 881172 922933 987053 546756 383287 351548 238477 357132 89016 [7,] 391842 975923 1022175 1093189 605548 424501 389349 264121 395534 98588 [8,] 469648 1169707 1225143 1310258 725788 508792 466660 316566 474073 118164 [9,] 390561 972733 1018834 1089616 603569 423113 388076 263257 394241 98266 [10,] 344014 856804 897410 959756 531636 372687 341826 231882 347255 86555 Of course, you can also extract fitted.values from the GLM stored in poi_glm. What is the difference with our own coded approach? fitted.values(poi_glm) 1 2 3 4 5 6 7 8 9 10 270061 672617 704494 753438 417350 292571 268344 182035 272606 67948 11 12 13 14 15 16 17 18 19 20 376125 936779 981176 1049342 581260 407474 373732 253527 379669 372325 21 22 23 24 25 26 27 28 29 30 927316 971264 1038741 575388 403358 369957 250966 366724 913365 956652 31 32 33 34 35 36 37 38 39 40 1023114 566731 397290 364391 336287 837559 877254 938200 519695 364316 41 42 43 44 45 46 47 48 49 50 353798 881172 922933 987053 546756 391842 975923 1022175 1093189 469648 51 52 53 54 55 1169707 1225143 390561 972733 344014 # extract future future &lt;- row(orig_fits) + col(orig_fits) - 1 &gt; TT future [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE [5,] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE [6,] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE [7,] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE [8,] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [9,] FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE [10,] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE To conclude, you calculate the point estimates of the reserves (in total and per row) as obtained from the poi_glm fit. # compare reserves with CL reserves orig_reserve_tot &lt;- sum(orig_fits[future]) orig_reserve_row &lt;- numeric(TT-1) for(i in 2:TT){ orig_reserve_row[i-1] &lt;- sum(orig_fits[i, (TT-i+2):TT]) } point_est_glm &lt;- data.frame(2:TT, orig_reserve_row) names(point_est_glm) &lt;- c(&quot;row&quot;, &quot;reserve&quot;) orig_reserve_tot [1] 18680856 point_est_glm row reserve 1 2 94634 2 3 469511 3 4 709638 4 5 984889 5 6 1419459 6 7 2177641 7 8 3920301 8 9 4278972 9 10 4625811 Exercise: compare the point estimates with the results stored in object M. What do you conclude? now redo the analysis, use family = quasipoisson to do overdispersed Poisson instead of regular Poisson regression. 6.5 Bootstrap analysis Starting from the results of an overdispersed Poisson GLM you can now code a bootstrap analysis to simulate from the predictive distribution of the outstanding reserve. Here is the preparatory work. # OD Poisson GLM of Xij versus i+j poi_glm_od &lt;- glm(Xij ~ i + j, quasipoisson(link=log), data = my_triangle_data) summary(poi_glm_od) Call: glm(formula = Xij ~ i + j, family = quasipoisson(link = log), data = my_triangle_data) Deviance Residuals: Min 1Q Median 3Q Max -464.9 -123.7 -21.7 116.2 494.3 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 12.50640 0.17292 72.32 &lt; 2e-16 *** i2 0.33127 0.15354 2.16 0.0377 * i3 0.32112 0.15772 2.04 0.0492 * i4 0.30596 0.16074 1.90 0.0650 . i5 0.21932 0.16797 1.31 0.1999 i6 0.27008 0.17076 1.58 0.1225 i7 0.37221 0.17445 2.13 0.0398 * i8 0.55333 0.18653 2.97 0.0053 ** i9 0.36893 0.23918 1.54 0.1317 i10 0.24203 0.42756 0.57 0.5749 j2 0.91253 0.14885 6.13 4.7e-07 *** j3 0.95883 0.15257 6.28 2.9e-07 *** j4 1.02600 0.15688 6.54 1.3e-07 *** j5 0.43528 0.18391 2.37 0.0234 * j6 0.08006 0.21477 0.37 0.7115 j7 -0.00638 0.23829 -0.03 0.9788 j8 -0.39445 0.31029 -1.27 0.2118 j9 0.00938 0.32025 0.03 0.9768 j10 -1.37991 0.89669 -1.54 0.1326 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 52602) Null deviance: 10699464 on 54 degrees of freedom Residual deviance: 1903014 on 36 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 4 # extract Pearson residuals Prs.resid &lt;- (my_triangle_data$Xij - fitted(poi_glm_od))/sqrt(fitted(poi_glm_od)) # get estimate dispersion parameter n [1] 55 p &lt;- 2*TT-1 p [1] 19 phi.P &lt;- sum(Prs.resid^2)/(n-p) phi.P [1] 52601 # adjust the residuals for bias in the same way as scale parameter Adj.Prs.resid &lt;- Prs.resid*sqrt(n/(n-p)) And here starts the bootstrap loop, see Katrien’s lecture notes for explanation and schematic overview. # initialize the random number generator set.seed(6345789) # i and j as factor variables i &lt;- rep(1:TT, TT:1); i &lt;- as.factor(i) j &lt;- sequence(TT:1); j &lt;- as.factor(j) # run the bootstrap loop many times, eg 10000 times nBoot &lt;- 10000 payments &lt;- numeric(nBoot) for(boots in 1:nBoot){ ## start of the bootstrap loop # Step 1: resample from the adjusted residuals, with replacement Ps.Xij &lt;- sample(Adj.Prs.resid, n, replace=TRUE) # Step 2: using this set of residuals and the estimated values of \\hat{\\mu_{ij}} # create a new suitable pseudo-history Ps.Xij &lt;- Ps.Xij*sqrt(fitted(poi_glm_od))+fitted(poi_glm_od) Ps.Xij &lt;- pmax(Ps.Xij, 0) # Step 3: from this history, estimate \\alpha_i, beta_j Ps.CL &lt;- glm(Ps.Xij ~ i + j, quasipoisson) coefs &lt;- exp(as.numeric(coef(Ps.CL))) Ps.alpha &lt;- c(1, coefs[2:TT])*coefs[1] Ps.beta &lt;- c(1, coefs[(TT+1):(2*TT-1)]) # Step 4: compute fitted values, use sum of future part as an estimate of the reserve Ps.fits &lt;- Ps.alpha%*%t(Ps.beta) Ps.reserve &lt;- sum(Ps.fits[future]) # Step 5: Ps.totpayments &lt;- phi.P*rpois(1, Ps.reserve/phi.P) # Step 6: payments[boots] &lt;- Ps.totpayments } min(payments) [1] 7416792 max(payments) [1] 31981628 You extract useful summaries from the results stored in payments as follows. quantile(payments, c(0.5, 0.75, 0.9, 0.95, 0.99), na.rm = TRUE) 50% 75% 90% 95% 99% 18673483 20672335 22671187 23986221 26826694 mean(payments) [1] 18848278 sd(payments) [1] 2968997 And plots d &lt;- data.frame(payments) library(ggplot2) ggplot(data=d, aes(payments)) + geom_histogram(col = &quot;black&quot;, fill = &quot;blue&quot;, alpha = 0.2) + theme_bw() + labs(title = &quot;Histogram for Bootstrap Reserves&quot;, x=&quot;Reserves&quot;, y=&quot;Count&quot;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The ChainLadder package also has useful functions to set up bootstrap simulations. set.seed(1) B &lt;- BootChainLadder(GenIns, R = 10000, process.distr = &quot;od.pois&quot;) B BootChainLadder(Triangle = GenIns, R = 10000, process.distr = &quot;od.pois&quot;) Latest Mean Ultimate Mean IBNR IBNR.S.E IBNR 75% IBNR 95% 1 3,901,463 3,901,463 0 0 0 0 2 5,339,085 5,435,471 96,386 113,603 149,138 312,621 3 4,909,315 5,380,702 471,387 217,453 598,149 864,988 4 4,588,268 5,305,145 716,877 261,068 876,808 1,179,647 5 3,873,311 4,864,352 991,041 307,441 1,179,230 1,542,424 6 3,691,712 5,126,756 1,435,044 380,790 1,673,438 2,107,963 7 3,483,130 5,673,303 2,190,173 497,466 2,509,815 3,058,627 8 2,864,498 6,815,226 3,950,728 796,044 4,445,624 5,355,080 9 1,363,294 5,684,146 4,320,852 1,064,740 4,952,187 6,209,475 10 344,014 5,043,687 4,699,673 2,018,504 5,899,153 8,156,800 Totals Latest: 34,358,090 Mean Ultimate: 53,230,252 Mean IBNR: 18,872,162 IBNR.S.E 3,001,662 Total IBNR 75%: 20,761,090 Total IBNR 95%: 24,064,106 plot(B) # some attributes o B$Triangle dev origin 1 2 3 4 5 6 7 8 9 1 357848 1124788 1735330 2218270 2745596 3319994 3466336 3606286 3833515 2 352118 1236139 2170033 3353322 3799067 4120063 4647867 4914039 5339085 3 290507 1292306 2218525 3235179 3985995 4132918 4628910 4909315 NA 4 310608 1418858 2195047 3757447 4029929 4381982 4588268 NA NA 5 443160 1136350 2128333 2897821 3402672 3873311 NA NA NA 6 396132 1333217 2180715 2985752 3691712 NA NA NA NA 7 440832 1288463 2419861 3483130 NA NA NA NA NA 8 359480 1421128 2864498 NA NA NA NA NA NA 9 376686 1363294 NA NA NA NA NA NA NA 10 344014 NA NA NA NA NA NA NA NA dev origin 10 1 3901463 2 NA 3 NA 4 NA 5 NA 6 NA 7 NA 8 NA 9 NA 10 NA B$f [1] 3.491 1.747 1.457 1.174 1.104 1.086 1.054 1.077 1.018 1.000 B$IBNR.Totals[1:10] [1] 21398909 17870368 17597508 18428981 19512320 19606687 15880725 22244763 [9] 19325943 17354310 6.6 More info Recent papers on reserving by Katrien: “A time change strategy to model reporting delay dynamics in claims reserving”, avalailable here “An EM algorithm to model the occurrence of events subject to a reporting delay” available here. "]
]
